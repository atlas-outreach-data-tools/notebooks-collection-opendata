{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to rediscover the Higgs boson yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- THE LEPTON ENERGIES IS NOW `lep_e` instead of `lep_E`\n",
    "- everything is in GeV now (lep pt, lep E)\n",
    "- change name for section on data_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../../images/ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern 2025 beta release to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Higgs boson? \n",
    "The Higgs boson is a fundamental particle predicted by the Standard Model. \n",
    "It is a manifestation of the Higgs field,\n",
    "    which gives mass to the fundamental particles.\n",
    "However,\n",
    "    it is incredibly hard to produce.\n",
    "At the LHC, \n",
    "    a Higgs particle is produced about once every 10 billion collisions!\n",
    "This tiny fraction makes it very difficult to detect.\n",
    "Nevertheless, \n",
    "    after years of data collection, \n",
    "    the Higgs boson was finally discovered in 2012 by CMS and ATLAS experiments at CERN.\n",
    "In this tutorial, \n",
    "    we shall be following their example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the Higgs\n",
    "This analysis loosely follows the paper on the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1).\n",
    "\n",
    "The Higgs boson can be produced in many different ways. \n",
    "In particle physics, \n",
    "    we describe these production modes using Feynman diagrams.\n",
    "These diagrams allow us to visualise particle processes while also acting as powerful tools for calculations.\n",
    "See [here](https://cds.cern.ch/record/2759490/files/Feynman%20Diagrams%20-%20ATLAS%20Cheat%20Sheet.pdf) for more information on Feynman diagrams.\n",
    "\n",
    "There are four main production modes of the Higgs boson, and their respective Feynman diagrams:\n",
    "1. Gluon-gluon fusion (top left)\n",
    "2. Vector boson fusion (top right)\n",
    "3. Vector boson bremsstrahlung (bottom left)\n",
    "4. Top-antitop fusion (bottom right)\n",
    "\n",
    "<CENTER><img src=\"images/ImagesHiggs/ggH.png\" style=\"width:40%\"> <img src=\"images/ImagesHiggs/VBFH.png\" style=\"width:35%\"></CENTER>\n",
    "<CENTER><img src=\"images/ImagesHiggs/WH.png\" style=\"width:40%\"> <img src=\"images/ImagesHiggs/ttbarfusion.png\" style=\"width:35%\"></CENTER>\n",
    "\n",
    "The Higgs has a very short lifetime,\n",
    "    on the order of $10^{-22} \\,\\text{s}$.\n",
    "It decays extremely quickly after production,\n",
    "    so there is no hope of directly detecting the particle.\n",
    "Nevertheless,\n",
    "    we can use the Standard Model to predict its \n",
    "    decay products: photons, Z bosons, quarks, etc.,\n",
    "    all with different probabilities.\n",
    "These **decay channels** can be used to identify the Higgs boson.\n",
    "In this notebook, \n",
    "    we'll be looking at one particular decay channel:\n",
    "$$H \\rightarrow ZZ^* \\rightarrow \\ell\\ell\\ell\\ell$$\n",
    "\n",
    "<CENTER><img src=\"images/ImagesHiggs/HZZ_feynman.png\" style=\"width:40%\"></CENTER>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to this as our desired **signal**.\n",
    "Ideally,\n",
    "    we would search for collisions which yield four leptons as products and this would tell us that a Higgs boson is present.\n",
    "Unfortunately,\n",
    "    in addition to our signal,\n",
    "    there are many other **background** processes that lead to four reconstructed leptons in the final state. \n",
    "The main background is $ZZ^*  \\to \\ell\\ell\\ell\\ell$,\n",
    "    where decay products have the same properties as those in the Higgs decay. \n",
    "This is known as an irreducible background. \n",
    "<CENTER><img src=\"images/ImagesHiggs/ZZllll.png\" style=\"width:40%\"></CENTER>\n",
    "\n",
    "We can get around this by accounting for the total invariant mass of the lepton products. \n",
    "We know through conservation of energy and momentum that the invariant mass of the products must be equal to the Higgs mass, \n",
    "    while other background processes will have different invariant masses. \n",
    "Our last step would be to plot the invariant mass of each event and spot the peak in mass around $125\\, \\text{GeV}$ , which corresponds to the mass of the Higgs boson. \n",
    "\n",
    "We also have background contributions from $Z +$ jets and top-anti top processes, \n",
    "    where additional charged leptons can arise either from semi-leptonic decays of heavy flavour or light flavour jets misidentified as leptons.\n",
    "These backgrounds are difficult to remove completely. \n",
    "\n",
    "<CENTER><img src=\"images/ImagesHiggs/Zllll.png\" style=\"width:30%\"><img src=\"images/ImagesHiggs/ttbar.png\" style=\"width:30%\"></CENTER>\n",
    "\n",
    "For such processes,\n",
    "    we will attempt to distinguish them from the Higgs decay using the properties of the leptons.\n",
    "Because the Higgs is a neutral particle with zero lepton number,\n",
    "    the lepton products from its decay must sum to zero charge and zero lepton numbers.\n",
    "Thus, \n",
    "    we can cut away all data with products that do not have these properties.\n",
    "These cuts increase the ratio of our signal to the reducible background.\n",
    "\n",
    "Note: $Z^*$ refers to a $Z$ boson that is off its mass shell. \n",
    "This means that its mass is not fixed to the $91 \\, \\text{GeV}$ of a typical $Z$ boson. \n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. Learn to process large data sets using cuts\n",
    "2. Understand some general principles of a particle physics analysis\n",
    "3. Discover the Higgs boson!\n",
    "\n",
    "See [here](https://cds.cern.ch/record/2800577/files/Signal%20and%20Background%20Physics%20Cheat%20Sheet.pdf) for more information on signals and backgrounds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Python notebook\n",
    "A Python notebook consists of cell blocks, \n",
    "    each containing lines of Python code.\n",
    "Each cell can be run independently of each other,\n",
    "    yielding respective outputs below the cells.\n",
    "Conventionally,\n",
    "    cells are run in order from top to bottom.\n",
    "\n",
    "\n",
    "- To run the whole notebook, in the top menu click Cell $\\to$ Run All.\n",
    "\n",
    "- To propagate a change you've made to a piece of code, click Cell $\\to$ Run All Below.\n",
    "\n",
    "- You can also run a single code cell, by clicking Cell $\\to$ Run Cells, or using the keyboard shortcut Shift+Enter.\n",
    "\n",
    "For more information, \n",
    "    refer to [here](https://www.codecademy.com/article/how-to-use-jupyter-notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATLAS Open Data Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First time package installation on your computer (not needed on mybinder)\n",
    "This first cell installs the required python packages.\n",
    "It only needs to be run the first time you open this notebook on your computer. \n",
    "If you close Jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If this is opened on mybinder, you don't need to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "# update the pip package installer\n",
    "# !{sys.executable} -m pip install --upgrade --user pip\n",
    "# install required packages\n",
    "# !{sys.executable} -m pip install --upgrade --user uproot awkward vector numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to import a number of packages to help us:\n",
    "* `numpy`: provides numerical calculations such as histogramming\n",
    "* `matplotlib`: common tool for making plots, figures, images, visualisations\n",
    "* `uproot`: processes `.root` files typically used in particle physics into data formats used in python\n",
    "* `awkward`: introduces `awkward` arrays, a format that generalizes `numpy` to nested data with possibly variable length lists\n",
    "* `vector`: to allow vectorized 4-momentum calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib_inline # to edit the inline plot format\n",
    "#matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg') # to make plots in pdf (vector) format\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "import uproot # for reading .root files\n",
    "import awkward as ak # to represent nested data in columnar format\n",
    "import vector # for 4-momentum calculations\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit definitions, as stored in the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeV = 0.001\n",
    "GeV = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [atlasopenmagic](https://opendata.atlas.cern/docs/data/atlasopenmagic) to access the open data directly from the ATLAS OpenData Portal so no need to download any samples. First we need to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install atlasopenmagic==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the module and load the release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atlasopenmagic as atom\n",
    "atom.available_releases()\n",
    "atom.set_release('2025e-13tev-beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Reading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to read some of the data from the open dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 36.6 # fb-1 # data size of the full release \n",
    "fraction = 1.0 # reduce this is if you want the code to run quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the skim to use for the analysis\n",
    "skim = \"exactly4lep\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenient naming and identification purposes,\n",
    "    we define a dictionary which stores all the important names of the samples we want to pull from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : atom.get_urls_data(skim),\n",
    "    },   \n",
    "    r'Background $Z,t\\bar{t},t\\bar{t}+V,VVV$' : { # Z+t+vvv\n",
    "        'list' : [atom.get_urls('410470',skim)[0], #'410470.PhPy8EG_A14_ttbar_hdamp258p75_nonallhad.exactly4lep',\n",
    "                  atom.get_urls('410155',skim)[0], #'410155.aMcAtNloPythia8EvtGen_MEN30NLO_A14N23LO_ttW.exactly4lep',\n",
    "                  atom.get_urls('410218',skim)[0], #'410218.aMcAtNloPythia8EvtGen_MEN30NLO_A14N23LO_ttee.exactly4lep',\n",
    "                  atom.get_urls('410219',skim)[0], #'410219.aMcAtNloPythia8EvtGen_MEN30NLO_A14N23LO_ttmumu.exactly4lep',\n",
    "                  atom.get_urls('412043',skim)[0], #'412043.aMcAtNloPythia8EvtGen_A14NNPDF31_SM4topsNLO.exactly4lep',\n",
    "                  atom.get_urls('364243',skim)[0], #'364243.Sherpa_222_NNPDF30NNLO_WWZ_4l2v_EW6.exactly4lep',\n",
    "                  atom.get_urls('364242',skim)[0], #'364242.Sherpa_222_NNPDF30NNLO_WWW_3l3v_EW6.exactly4lep',\n",
    "                  atom.get_urls('364246',skim)[0], #'364246.Sherpa_222_NNPDF30NNLO_WZZ_3l3v_EW6.exactly4lep',\n",
    "                  atom.get_urls('364248',skim)[0], #'364248.Sherpa_222_NNPDF30NNLO_ZZZ_4l2v_EW6.exactly4lep',\n",
    "                  atom.get_urls('700320',skim)[0], #'700320.Sh_2211_Zee_maxHTpTV2_BFilter.exactly4lep',\n",
    "                  atom.get_urls('700321',skim)[0], #'700321.Sh_2211_Zee_maxHTpTV2_CFilterBVeto.exactly4lep',\n",
    "                  atom.get_urls('700322',skim)[0], #'700322.Sh_2211_Zee_maxHTpTV2_CVetoBVeto.exactly4lep',\n",
    "                  atom.get_urls('700323',skim)[0], #'700323.Sh_2211_Zmumu_maxHTpTV2_BFilter.exactly4lep',\n",
    "                  atom.get_urls('700324',skim)[0], #'700324.Sh_2211_Zmumu_maxHTpTV2_CFilterBVeto.exactly4lep',\n",
    "                  atom.get_urls('700325',skim)[0], #'700325.Sh_2211_Zmumu_maxHTpTV2_CVetoBVeto.exactly4lep'\n",
    "                 ],\n",
    "        'color' : \"#6b59d3\" # purple\n",
    "    },\n",
    "    r'Background $ZZ^{*}$' : { # ZZ\n",
    "        'list' : [atom.get_urls('700600',skim)[0], #'700600.Sh_2212_llll.exactly4lep',\n",
    "                  atom.get_urls('700601',skim)[0], #'700601.Sh_2212_lllv.exactly4lep'\n",
    "                 ],\n",
    "        'color' : \"#ff0000\" # red\n",
    "    },\n",
    "    r'Signal ($m_H$ = 125 GeV)' : { # H -> ZZ -> llll\n",
    "        'list' : [atom.get_urls('345060',skim)[0], #'345060.PowhegPythia8EvtGen_NNLOPS_nnlo_30_ggH125_ZZ4l.exactly4lep',\n",
    "                  atom.get_urls('346228',skim)[0], #'346228.PowhegPy8EG_NNPDF30_AZNLOCTEQ6L1_VBFH125_ZZ4lep_notau.exactly4lep',\n",
    "                  atom.get_urls('346311',skim)[0], #'346311.PowhegPythia8EvtGen_NNPDF30_AZNLO_WpH125J_Wincl_H_incl_MINLO.exactly4lep',\n",
    "                  atom.get_urls('346312',skim)[0], #'346312.PowhegPythia8EvtGen_NNPDF30_AZNLO_WmH125J_Wincl_H_incl_MINLO.exactly4lep',\n",
    "                  atom.get_urls('346340',skim)[0], #'346340.PowhegPy8EG_A14NNPDF23_NNPDF30ME_ttH125_ZZ4l_allhad.exactly4lep',\n",
    "                  atom.get_urls('346341',skim)[0], #'346341.PowhegPy8EG_A14NNPDF23_NNPDF30ME_ttH125_ZZ4l_semilep.exactly4lep',\n",
    "                  atom.get_urls('346342',skim)[0], #'346342.PowhegPy8EG_A14NNPDF23_NNPDF30ME_ttH125_ZZ4l_dilep.exactly4lep'          \n",
    "                ],\n",
    "        'color' : \"#00cdff\" # light blue\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key named `data` refers to the event information collected from real experiments,\n",
    "    while the `Background` and `Signal` keys refer to Monte-Carlo (MC) simulations of the ATLAS experiments.\n",
    "Both real data and MC data will then be analysed and compared together to discover the Higgs! \n",
    "\n",
    "Let's try accessing `data15_periodD` in the CERN database URL as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the samples dict for the key 'data'\n",
    "print(samples['data'])\n",
    "\n",
    "# We shall use the first entry in 'list', 'data15_periodD'\n",
    "value = samples['data']['list'][0]\n",
    "print(f\"{value = }\")\n",
    "\n",
    "# This is now appended to our file path to retrieve the data_A.4lep.root file\n",
    "data15_periodD = value #path + \"Data/\" + value + \".root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall try opening the `data15_periodD` file to see what is inside.\n",
    "In the file (called a `tree`),\n",
    "    there are 39 entries, \n",
    "    one for each event.\n",
    "In each event,\n",
    "    a dictionary stores the all relevant information as keys, such as the event number (`eventNumber`), lepton transverse momentum (`lep_pt`), etc.  \n",
    "Details on the variables in the dictionary can be viewed [here](https://opendata.atlas.cern/docs/data/for_education/13TeV25_details#variable-list).\n",
    "\n",
    "More information on trees can be viewed [here](https://masonproffitt.github.io/uproot-tutorial/03-trees/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the file from the online database (\":analysis\" opens the tree in a desired manner)\n",
    "tree = uproot.open(data15_periodD + \":analysis\")\n",
    "\n",
    "# There are 39 entries in the tree\n",
    "print(tree.num_entries)\n",
    "\n",
    "# We can view all the information stored in the tree using the .keys() method.\n",
    "print(tree.keys())\n",
    "\n",
    "# We can also view the entire tree using the .arrays() method\n",
    "# This generates a 39-entry list of dictionaries\n",
    "print(tree.arrays()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we'd like to see the lepton energies. \n",
    "We can access this from our tree using the key `lep_e`. \n",
    "Also, \n",
    "    from this point on we shall be manipulating our tree arrays using the `awkward` library.\n",
    "We can use `library=\"ak\"` in the argument of the `.arrays()` method to use this library.\n",
    "If you ever see `library=\"ak\"` in the code,\n",
    "    it means that the array is output as an `awkward` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"lep_e\"].arrays(library=\"ak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis, \n",
    "    not all the information in the tree is important.\n",
    "We can store the important variables in a list and retrieve them from the tree later on.\n",
    "As it turns out, \n",
    "    we will need the following set of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what variables are important to our analysis\n",
    "variables = ['lep_pt','lep_eta','lep_phi','lep_e','lep_charge','lep_type','trigE','trigM','lep_isTrigMatched',\n",
    "            'lep_isLooseID','lep_isMediumID','lep_isLooseIso','lep_type']\n",
    "\n",
    "# To see all the data for our given variables\n",
    "# for data in tree.iterate(variables, library=\"ak\"):\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to access the information in the `data15_periodD` tree,\n",
    "    we can begin analysis.\n",
    "As mentioned in the introduction,\n",
    "    there are two key steps to be completed for each event entry:\n",
    "1. **Cuts** - we need to account for lepton selection rules in the event. \n",
    "In the [paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X), \n",
    "    it is stated that we must\n",
    "\"[select] two pairs of isolated leptons, each of which is comprised of two leptons with the **same flavour** and **opposite charge**\".\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event.\n",
    "We need to filter the data such that in each event, there are pairs of leptons of the **same lepton type** (`lep_type`) and summing to **zero lepton charge** (`lep_charge`).\n",
    "\n",
    "2. **Mass calculation** - the data to be plotted is the 4-lepton invariant mass, which can be found using the equation: $$m_\\text{4l} = \\sqrt{E^2_\\text{tot}-\\mathbf{p}_\\text{tot}\\cdot\\mathbf{p}_\\text{tot}}$$\n",
    "in units where $c=1$.\n",
    "$E_\\text{tot}$ is the total energy and $\\mathbf{p}_\\text{tot}$ is the total momentum.\n",
    "This calculation is performed using the vector array method `.M` on the sum of lepton 4-momenta (`lep_pt`,`lep_eta`,`lep_phi`,`lep_E`).\n",
    "\n",
    "From this,\n",
    "    we can see why we chose those six important variables earlier. \n",
    "The physical reasoning for why we perform these steps is encapsulated in the idea of **conservation laws**.\n",
    "You may read more [here](https://cds.cern.ch/record/2759491/files/Conservation%20Laws%20-%20ATLAS%20Physics%20Cheat%20Sheet.pdf).\n",
    "\n",
    "Let's try to perform this two-step analysis for one event in `data15_periodD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to get data from files.\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This selects the first entry of the tree\n",
    "entry = tree.arrays(library=\"ak\")[:1] # EXPLAIN LIBRARY\n",
    "\n",
    "# Cut lepton type (electron type is 11,  muon type is 13)\n",
    "lep_type = entry['lep_type']\n",
    "sum_lep_type = lep_type[:, 0] + lep_type[:, 1] + lep_type[:, 2] + lep_type[:, 3]\n",
    "lep_type_cut_bool = (sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52)\n",
    "print(f\"Cut for lepton type? {lep_type_cut_bool}\") # True means we should remove this entry (lepton type does not match)\n",
    "\n",
    "# Cut lepton charge\n",
    "# first lepton in each event is [:, 0], 2nd lepton is [:, 1] etc\n",
    "lep_charge = entry['lep_charge']\n",
    "sum_lep_charge = lep_charge[:, 0] + lep_charge[:, 1] + lep_charge[:, 2] + lep_charge[:, 3] != 0\n",
    "print(f\"Cut for lepton charge? {sum_lep_charge}\") # True means we should remove this entry (sum of lepton charges is not equal to 0)\n",
    "\n",
    "# Calculate invariant mass of the 4-lepton state\n",
    "# [:, i] selects the i-th lepton in each event\n",
    "p4 = vector.zip({\"pt\": entry['lep_pt'], \"eta\": entry['lep_eta'], \"phi\": entry['lep_phi'], \"E\": entry['lep_e']})\n",
    "invariant_mass = (p4[:, 0] + p4[:, 1] + p4[:, 2] + p4[:, 3]).M # .M calculates the invariant mass\n",
    "print(f\"Invariant mass: {invariant_mass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our analysis, this entry should be removed because the lepton type does not match our requirements.\n",
    "We can turn these checks and calculations into a set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut lepton type (electron type is 11,  muon type is 13)\n",
    "def cut_lep_type(lep_type):\n",
    "    sum_lep_type = lep_type[:, 0] + lep_type[:, 1] + lep_type[:, 2] + lep_type[:, 3]\n",
    "    lep_type_cut_bool = (sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52)\n",
    "    return lep_type_cut_bool # True means we should remove this entry (lepton type does not match)\n",
    "\n",
    "# Cut lepton charge\n",
    "def cut_lep_charge(lep_charge):\n",
    "    # first lepton in each event is [:, 0], 2nd lepton is [:, 1] etc\n",
    "    sum_lep_charge = lep_charge[:, 0] + lep_charge[:, 1] + lep_charge[:, 2] + lep_charge[:, 3] != 0\n",
    "    return sum_lep_charge # True means we should remove this entry (sum of lepton charges is not equal to 0)\n",
    "\n",
    "# Calculate invariant mass of the 4-lepton state\n",
    "# [:, i] selects the i-th lepton in each event\n",
    "def calc_mass(lep_pt, lep_eta, lep_phi, lep_e):\n",
    "    p4 = vector.zip({\"pt\": lep_pt, \"eta\": lep_eta, \"phi\": lep_phi, \"E\": lep_e})\n",
    "    invariant_mass = (p4[:, 0] + p4[:, 1] + p4[:, 2] + p4[:, 3]).M # .M calculates the invariant mass\n",
    "    return invariant_mass\n",
    "\n",
    "\n",
    "def cut_trig_match(lep_trigmatch): \n",
    "    trigmatch = lep_trigmatch  \n",
    "    cut1 = ak.sum(trigmatch, axis=1) >= 1 \n",
    "    return cut1\n",
    "\n",
    "def cut_trig(trigE,trigM):\n",
    "    return trigE | trigM\n",
    "\n",
    "\n",
    "def ID_iso_cut(IDel,IDmu,isoel,isomu,pid): \n",
    "    thispid = pid \n",
    "    return (ak.sum(((thispid == 13) & IDmu & isomu) | ((thispid == 11) & IDel & isoel), axis=1) == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may verify on your own that these functions give the same outputs as the previous code block.\n",
    "Now, \n",
    "    we shall apply these functions over the entire data tree using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty list to hold all data for this sample\n",
    "sample_data = []\n",
    "\n",
    "# Perform the cuts for each data entry in the tree\n",
    "for data in tree.iterate(variables, library=\"ak\"): # the data will be in the form of an awkward array\n",
    "    # We can use data[~boolean] to remove entries from the data set\n",
    "    lep_type = data['lep_type']\n",
    "    data = data[~cut_lep_type(lep_type)]\n",
    "    lep_charge = data['lep_charge']\n",
    "    data = data[~cut_lep_charge(lep_charge)]\n",
    "\n",
    "    data['mass'] = calc_mass(data['lep_pt'], data['lep_eta'], data['lep_phi'], data['lep_e'])\n",
    "\n",
    "    # Append data to the whole sample data list\n",
    "    sample_data.append(data)\n",
    "\n",
    "# turns sample_data back into an awkward array\n",
    "data_A = ak.concatenate(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the data using Matplotlib. \n",
    "The data will be turned into a histogram,\n",
    "    with bins of width $2.5 \\,\\text{GeV}$.\n",
    "Note that much of the code written here is meant for the aesthetics of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis range of the plot\n",
    "xmin = 80 * GeV\n",
    "xmax = 250 * GeV\n",
    "\n",
    "# Histogram bin setup\n",
    "step_size = 2.5 * GeV\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "# Creating histogram from data\n",
    "data_x,_ = np.histogram(ak.to_numpy(data_A['mass']), \n",
    "                        bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                    label='Data') \n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ); # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great,\n",
    "    we managed to plot `data15_periodD`! \n",
    "Now, \n",
    "    we have not discussed how to deal with the Monte-Carlo simulation data,\n",
    "    or even what they are for. \n",
    "Let us explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Reading Monte-Carlo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the Standard Model, \n",
    "    we can do a set of randomised simulations to produce a set of theoretical data points to compare to our ATLAS data.\n",
    "These are known as Monte-Carlo(MC) simulations.\n",
    "There is one important change to be made to the MC data before we can compare them with our ATLAS data:\n",
    " - **Weights** - The MC data was computed in ideal circumstances. The real ATLAS detector has some inefficiencies, which we can account for by attributing the appropriate weight to each data point. The weight of a data point affects how it contributes to the histogram count for its bin.\n",
    "\n",
    "Let's open an MC file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We open an MC data file with sample value \"Zee\" using samples and infofile for reference of filenames\n",
    "print(samples[r'Background $Z,t\\bar{t},t\\bar{t}+V,VVV$'])\n",
    "value = samples[r'Background $Z,t\\bar{t},t\\bar{t}+V,VVV$'][\"list\"][0]\n",
    "\n",
    "# This is now appended to our file path to retrieve the root file\n",
    "background_ttbar_path = value\n",
    "\n",
    "# Accessing the file from the online database\n",
    "tree = uproot.open(background_ttbar_path + \":analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, \n",
    "    not all weights are important to our analysis. \n",
    "In our case, \n",
    "    these are:\n",
    "- `mcWeight` - specific Monte-Carlo weight associated with each event\n",
    "- `scaleFactor_PILEUP` - scale factor for pileup reweighting\n",
    "- `scaleFactor_ELE` - scale factor for electron efficiency\n",
    "- `scaleFactor_MUON`- scale factor for muon efficiency\n",
    "- `scaleFactor_LepTRIGGER` - scale factor for lepton triggers (TODO not around for new release)\n",
    "\n",
    "Scale factors are generally related to estimates of the efficiencies and resolutions of detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_variables = [\"filteff\",\"kfac\",\"xsec\",\"mcWeight\",\"ScaleFactor_PILEUP\", \"ScaleFactor_ELE\", \"ScaleFactor_MUON\", \"ScaleFactor_LepTRIGGER\"]\n",
    "\n",
    "# For example, see below for the weights corresponding to muon rejection\n",
    "tree[\"ScaleFactor_MUON\"].arrays(library = \"ak\")\n",
    "print(weight_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally,\n",
    "    there is a cross-section weight $w_\\sigma$ associated with each MC file.\n",
    "We define this variable `xsec_weight` below. \n",
    "This weight is meant to normalise the entire Monte-Carlo distribution based on the number of events in the data.\n",
    "This is its definition:\n",
    "$$ w_\\sigma = \\frac{\\int L \\text{d}t ~ \\sigma }{\\eta \\sum_i w_i } $$\n",
    "where $\\int L \\text{d}t$ is the integrated luminosity (`lumi`),\n",
    "    $\\sigma$ is the cross section (`info[\"xsec\"]`),\n",
    "    $\\eta$ is the filter efficiency of the MC generator,\n",
    "    and $\\sum_i w_i$ gives the sum of all weights (`info[\"sumw\"]`).\n",
    "When the integrated luminosity is multiplied by the cross section,\n",
    "    it gives a measure of the total number of events during a period of data taking.\n",
    "For `data_A`,\n",
    "    the integrated luminosity has a value of $0.5 \\,\\text{fb}^{-1}$.\n",
    "\n",
    "For more on cross sections and luminosities, \n",
    "    [see this cheatsheet](https://cds.cern.ch/record/2800578/files/Cross%20Section%20and%20Luminosity%20Physics%20Cheat%20Sheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,\n",
    "    with all the weights we've defined, \n",
    "    we will calculate a total weight for an event,\n",
    "    which is the collective product of all the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luminosity of data_D\n",
    "lumi = 36. #TODO match data D\n",
    "\n",
    "# Let's use the first event of our tree\n",
    "event = tree.arrays()[0]\n",
    "\n",
    "# Multiply all the important weights together\n",
    "total_weight = lumi * 1000 / event[\"sum_of_weights\"]\n",
    "for variable in weight_variables:\n",
    "    total_weight = total_weight * event[variable]\n",
    "print(f\"{total_weight = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation means that in our final histogram, \n",
    "    this event will be represented with $0.5943843296175445$ of a single count in the bin.\n",
    "We can encapsulate these calculations in a single function `calc_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(weight_variables, events):\n",
    "    total_weight = lumi * 1000 / events[\"sum_of_weights\"]\n",
    "    for variable in weight_variables:\n",
    "        total_weight = total_weight * abs(events[variable])\n",
    "    return total_weight\n",
    "\n",
    "# Verify that we get the same answer\n",
    "print(calc_weight(weight_variables, event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply the cuts as before to plot the MC data.\n",
    "The code is the same as before,\n",
    "    but we make sure to add in `weight_variables` to our `tree.iterate()`,\n",
    "    and we store the weights in each event using a new dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = []\n",
    "\n",
    "# Perform the cuts for each data entry in the tree\n",
    "for data in tree.iterate(variables + weight_variables+['sum_of_weights'], library=\"ak\"):\n",
    "    # Cuts\n",
    "    lep_type = data['lep_type']\n",
    "    data = data[~cut_lep_type(lep_type)]\n",
    "    lep_charge = data['lep_charge']\n",
    "    data = data[~cut_lep_charge(lep_charge)]\n",
    "    \n",
    "    # Invariant Mass\n",
    "    data['mass'] = calc_mass(data['lep_pt'], data['lep_eta'], data['lep_phi'], data['lep_e'])\n",
    "\n",
    "    # Store Monte Carlo weights in the data\n",
    "    data['totalWeight'] = calc_weight(weight_variables, data)\n",
    "    # data['totalWeight'] = calc_weight(data)\n",
    "\n",
    "    # Append data to the whole sample data list\n",
    "    sample_data.append(data)\n",
    "\n",
    "# turns sample_data back into an awkward array\n",
    "background_ttbar = ak.concatenate(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 2.5 * GeV\n",
    "\n",
    "mc_x = ak.to_numpy(background_ttbar[\"mass\"]) # define list to hold the Monte Carlo histogram entries\n",
    "mc_weights = ak.to_numpy(background_ttbar[\"totalWeight\"]) # define list to hold the Monte Carlo weights\n",
    "mc_colors = samples[r'Background $Z,t\\bar{t},t\\bar{t}+V,VVV$']['color'] # define list to hold the colors of the Monte Carlo bars\n",
    "mc_labels = r'Background $Z,t\\bar{t},t\\bar{t}+V,VVV$' # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "# main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "#                     fmt='ko', # 'k' means black and 'o' is for circles \n",
    "#                     label='Data') \n",
    "\n",
    "# plot the Monte Carlo bars\n",
    "mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                            weights=mc_weights, stacked=True, \n",
    "                            color=mc_colors, label=mc_labels )\n",
    "\n",
    "mc_x_tot = mc_heights[0] # stacked background MC y-axis value\n",
    "\n",
    "# calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "\n",
    "# plot the statistical uncertainty\n",
    "main_axes.bar(bin_centres, # x\n",
    "                2*mc_x_err, # heights\n",
    "                alpha=0.5, # half transparency\n",
    "                bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ); # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand all the steps of our analysis,\n",
    "    all that's left is to import the entire ATLAS data and implement it.\n",
    "The `samples` dictionary will be useful for this.\n",
    "\n",
    "We will loop over all values in the `samples` dictionary.\n",
    "Depending on whether it is a data sample or MC sample, \n",
    "    `fileString` will change,\n",
    "    which opens the correct file on the open data folder.\n",
    "As before, \n",
    "    the cuts, \n",
    "    mass calculations and MC weight calculations will be performed for each sample value,\n",
    "    and then stored in the array.\n",
    "The data will all be concatenated into `all_data` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set luminosity to 36.6 fb-1, data size of the full release \n",
    "lumi = 36.6 \n",
    "\n",
    "# Controls the fraction of all events analysed\n",
    "fraction = 1.0 # reduce this is if you want quicker runtime (implemented in the loop over the tree)\n",
    "\n",
    "# Define empty dictionary to hold awkward arrays\n",
    "all_data = {} \n",
    "\n",
    "# Loop over samples\n",
    "for s in samples: \n",
    "\n",
    "    # Print which sample is being processed\n",
    "    print('Processing '+s+' samples') \n",
    "\n",
    "    # Define empty list to hold data\n",
    "    frames = [] \n",
    "\n",
    "    # Loop over each file\n",
    "    for val in samples[s]['list']: \n",
    "        if s == 'data': \n",
    "            prefix = \"Data/\" # Data prefix\n",
    "        else: # MC prefix\n",
    "            prefix = \"MC/mc_\"\n",
    "        fileString = val\n",
    "\n",
    "        # start the clock\n",
    "        start = time.time()\n",
    "        print(\"\\t\"+val+\":\") \n",
    "\n",
    "        # Open file\n",
    "        tree = uproot.open(fileString + \":analysis\")\n",
    "        \n",
    "        sample_data = []\n",
    "\n",
    "        # Loop over data in the tree\n",
    "        for data in tree.iterate(variables + weight_variables + [\"sum_of_weights\", \"lep_n\"], \n",
    "                                 library=\"ak\", \n",
    "                                 entry_stop=tree.num_entries*fraction):#, # process up to numevents*fraction\n",
    "                                #  step_size = 10000000): \n",
    "\n",
    "            # Number of events in this batch\n",
    "            nIn = len(data) \n",
    "            \n",
    "            data = data[cut_trig(data.trigE, data.trigM)]\n",
    "            data = data[cut_trig_match(data.lep_isTrigMatched)]\n",
    "\n",
    "            # Record transverse momenta (see bonus activity for explanation)\n",
    "            data['leading_lep_pt'] = data['lep_pt'][:,0]\n",
    "            data['sub_leading_lep_pt'] = data['lep_pt'][:,1]\n",
    "            data['third_leading_lep_pt'] = data['lep_pt'][:,2]\n",
    "            data['last_lep_pt'] = data['lep_pt'][:,3]\n",
    "            \n",
    "            # Cuts on transverse momentum\n",
    "            data = data[data['leading_lep_pt'] > 20]\n",
    "            data = data[data['sub_leading_lep_pt'] > 15]\n",
    "            data = data[data['third_leading_lep_pt'] > 10]\n",
    "            \n",
    "            data = data[ID_iso_cut(data.lep_isLooseID, \n",
    "                                   data.lep_isMediumID,  \n",
    "                                   data.lep_isLooseIso, \n",
    "                                   data.lep_isLooseIso, \n",
    "                                   data.lep_type)]\n",
    "\n",
    "            # Number Cuts\n",
    "            #data = data[data['lep_n'] == 4]\n",
    "\n",
    "            # Lepton cuts\n",
    "\n",
    "            lep_type = data['lep_type']\n",
    "            data = data[~cut_lep_type(lep_type)]\n",
    "            lep_charge = data['lep_charge']\n",
    "            data = data[~cut_lep_charge(lep_charge)]\n",
    "            \n",
    "            # Invariant Mass\n",
    "            data['mass'] = calc_mass(data['lep_pt'], data['lep_eta'], data['lep_phi'], data['lep_e'])\n",
    "\n",
    "            # Store Monte Carlo weights in the data\n",
    "            if 'data' not in s: # Only calculates weights if the data is MC\n",
    "                data['totalWeight'] = calc_weight(weight_variables, data)\n",
    "                # data['totalWeight'] = calc_weight(data)\n",
    "\n",
    "            # Append data to the whole sample data list\n",
    "            sample_data.append(data)\n",
    "\n",
    "            if not 'data' in val:\n",
    "                nOut = sum(data['totalWeight']) # sum of weights passing cuts in this batch \n",
    "            else:\n",
    "                nOut = len(data)\n",
    "\n",
    "            elapsed = time.time() - start # time taken to process\n",
    "            print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "\n",
    "        frames.append(ak.concatenate(sample_data)) \n",
    "\n",
    "    all_data[s] = ak.concatenate(frames) # dictionary entry is concatenated awkward arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x,_ = np.histogram(ak.to_numpy(all_data['data']['mass']), \n",
    "                        bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "signal_x = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)']['mass']) # histogram the signal\n",
    "signal_weights = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)'].totalWeight) # get the weights of the signal events\n",
    "signal_color = samples[r'Signal ($m_H$ = 125 GeV)']['color'] # get the colour for the signal bar\n",
    "\n",
    "mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "for s in samples: # loop over samples\n",
    "    if s not in ['data', r'Signal ($m_H$ = 125 GeV)']: # if not data nor signal\n",
    "        mc_x.append( ak.to_numpy(all_data[s]['mass']) ) # append to the list of Monte Carlo histogram entries\n",
    "        mc_weights.append( ak.to_numpy(all_data[s].totalWeight) ) # append to the list of Monte Carlo weights\n",
    "        mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "        mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "fig, main_axes = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                    label='Data') \n",
    "\n",
    "# plot the Monte Carlo bars\n",
    "mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                            weights=mc_weights, stacked=True, \n",
    "                            color=mc_colors, label=mc_labels )\n",
    "\n",
    "mc_x_tot = mc_heights[0][-1] # stacked background MC y-axis value\n",
    "\n",
    "# calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "\n",
    "# plot the signal bar\n",
    "signal_heights = main_axes.hist(signal_x, bins=bin_edges, bottom=mc_x_tot, \n",
    "                weights=signal_weights, color=signal_color,\n",
    "                label=r'Signal ($m_H$ = 125 GeV)')\n",
    "\n",
    "# plot the statistical uncertainty\n",
    "main_axes.bar(bin_centres, # x\n",
    "                2*mc_x_err, # heights\n",
    "                alpha=0.5, # half transparency\n",
    "                bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*2.0 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# Add text 'ATLAS Open Data' on plot\n",
    "plt.text(0.1, # x\n",
    "            0.93, # y\n",
    "            'ATLAS Open Data', # text\n",
    "            transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "            fontsize=16 ) \n",
    "\n",
    "# Add text 'for education' on plot\n",
    "plt.text(0.1, # x\n",
    "            0.88, # y\n",
    "            'for education', # text\n",
    "            transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "            style='italic',\n",
    "            fontsize=12 ) \n",
    "\n",
    "# Add energy and luminosity\n",
    "lumi_used = str(lumi*fraction) # luminosity to write on the plot\n",
    "plt.text(0.1, # x\n",
    "            0.82, # y\n",
    "            '$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', # text\n",
    "            transform=main_axes.transAxes,fontsize=16 ) # coordinate system used is that of main_axes\n",
    "\n",
    "# Add a label for the analysis carried out\n",
    "plt.text(0.1, # x\n",
    "            0.76, # y\n",
    "            r'$H \\rightarrow ZZ^* \\rightarrow 4\\ell$', # text \n",
    "            transform=main_axes.transAxes,fontsize=16 ) # coordinate system used is that of main_axes\n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False, fontsize=16 ) # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some analysis to study how significant the signal is compared to the background. \n",
    "One method is to check a quantity known as the signal significance $S$,\n",
    "    which is defined by \n",
    "$$ S = \\frac{N_\\text{sig}}{\\sqrt{N_\\text{bg}}}  $$\n",
    "where $ N_\\text{sig} $ and $N_\\text{bg}$ are the number of signal and background points respectively.\n",
    "A larger $S$ represents a better signal-to-background ratio,\n",
    "    and a more significant signal peak.\n",
    "To calculate $N_\\text{sig}$, \n",
    "    we can look at the plot and sum over the number of events of our Monte-Carlo signal.\n",
    "The signal range roughly corresponds to the bins from $115 \\,\\text{GeV}$ to $130 \\, \\text{GeV}$.\n",
    "$N_\\text{bg}$ then corresponds to the number of background events in those same bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal stacked height\n",
    "signal_tot = signal_heights[0] + mc_x_tot\n",
    "\n",
    "# Peak of signal\n",
    "print(signal_tot[18])\n",
    "\n",
    "# Neighbouring bins\n",
    "print(signal_tot[17:20])\n",
    "\n",
    "# Signal and background events\n",
    "N_sig = signal_tot[17:20].sum()\n",
    "N_bg = mc_x_tot[17:20].sum()\n",
    "\n",
    "# Signal significance calculation\n",
    "signal_significance = N_sig/np.sqrt(N_bg + 0.3 * N_bg**2) # EXPLAIN THE 0.3\n",
    "print(f\"\\nResults:\\n{N_sig = }\\n{N_bg = }\\n{signal_significance = }\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

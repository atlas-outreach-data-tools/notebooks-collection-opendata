{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoPapadopoulos/ttbar_analysis/blob/main/ttbar_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9CDWygzhxRE"
      },
      "source": [
        "#**How to Rediscover the Top-Antitop $t\\bar{t}$ Quark Pair Production Yourself!**\n",
        "\n",
        "This notebook uses ATLAS Open Data to guide you through the steps needed to rediscover the production of top-antitop quark pairs $t\\bar{t}$ in high-energy proton-proton collisions at the Large Hadron Collider (LHC).\n",
        "\n",
        "ATLAS Open Data provides open access to proton-proton collision data collected by the ATLAS experiment at the LHC. These datasets are made available for educational purposes, making them ideal for high-school, undergraduate, and postgraduate students interested in learning about particle physics.\n",
        "\n",
        "# **What Are Notebooks?**\n",
        "\n",
        "Notebooks are interactive web applications that allow you to create and share documents that contain:\n",
        "\n",
        "1. **Live code:** Write and execute code in real-time, making adjustments as you go.\n",
        "\n",
        "2. **Visualizations:** Create plots, histograms, and other graphical representations of your data to better understand the underlying physics.\n",
        "\n",
        "3. **Narrative text:** Include explanations, descriptions, and commentary to guide yourself or others through the analysis.\n",
        "\n",
        "\n",
        "# **The Goal: Rediscovering $t\\bar{t}$ Production**\n",
        "\n",
        "By following this notebook, you will perform a $t\\bar{t}$ analysis, aiming to identify and study the production of top-antitop quark pairs. The process involves applying a series of selection cuts and analysis techniques to increase the ratio of signal (events where $t\\bar{t}$ pairs are produced) to background (other processes that can mimic the signal).\n",
        "\n",
        "A key part of this analysis is focusing on the semileptonic decay of the $t\\bar{t}$ pair. In this decay mode, one top quark decays into a W boson and a b-quark, with the W boson further decaying into a charged lepton and a neutrino, while the other top quark decays hadronically (into jets).This can be represented as:\n",
        "* $ t\\bar{t} \\rightarrow lvb\\bar{b}q\\bar{q}$\n",
        "\n",
        "where:\n",
        "\n",
        "* $l$ is a lepton,\n",
        "* $v$ is a neutrino,\n",
        "* $b$ is a b-tagged jet,\n",
        "* $q$ is a jet.\n",
        "\n",
        "\n",
        "**Contents:**\n",
        "* Running a Jupyter notebook\n",
        "* To setup\n",
        "* Explanation of Key Parameters in the $t\\bar{t}$ Analysis Code\n",
        "* Samples\n",
        "* Weight in Particle Physics Analysis\n",
        "* Introduction to Event Selection Cuts in $t\\bar{t}$ Analysis\n",
        "* Introduction to Mass Reconstruction in $t\\bar{t}$ Analysis\n",
        "* Data Processing and Event Selection Function\n",
        "* Data Retrieval in $t\\bar{t}$ Analysis\n",
        "* Data Processing\n",
        "* Data Aggregation with Loops\n",
        "* Combining Mass and Weight Lists\n",
        "* Plotting\n",
        "\n",
        "\n",
        "# **Running a Jupyter notebook**\n",
        "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
        "\n",
        "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
        "\n",
        "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "63X2lXpgywvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5705c632-ce36-4196-d8b7-7218b917ccdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting atlasopenmagic\n",
            "  Downloading atlasopenmagic-0.6.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from atlasopenmagic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from atlasopenmagic) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (2025.4.26)\n",
            "Downloading atlasopenmagic-0.6.0-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.4/253.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: atlasopenmagic\n",
            "Successfully installed atlasopenmagic-0.6.0\n",
            "Collecting uproot\n",
            "  Downloading uproot-5.6.2-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting awkward>=2.4.6 (from uproot)\n",
            "  Downloading awkward-2.8.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: cramjam>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.10.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from uproot) (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot) (24.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from uproot) (3.5.0)\n",
            "Collecting awkward-cpp==46 (from awkward>=2.4.6->uproot)\n",
            "  Downloading awkward_cpp-46-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.21.0)\n",
            "Downloading uproot-5.6.2-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.7/375.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awkward-2.8.3-py3-none-any.whl (886 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.1/886.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awkward_cpp-46-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (638 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m638.7/638.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: awkward-cpp, awkward, uproot\n",
            "Successfully installed awkward-2.8.3 awkward-cpp-46 uproot-5.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install atlasopenmagic\n",
        "!pip install uproot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f477h5dJ-SFM"
      },
      "source": [
        "# **To setup**\n",
        "Cell -> Run All Below\n",
        "\n",
        "to be done every time you re-open this notebook\n",
        "\n",
        "We're going to be using a number of tools to help us:\n",
        "\n",
        "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
        "* awkward: lets us use efficiently the nested data in columnar format\n",
        "* pandas: lets us store data as dataframes, a format widely used in python\n",
        "* numpy: provides numerical calculations such as histogramming\n",
        "* matplotlib: common tool for making plots, figures, images, visualisations\n",
        "* concurrent.futures: Enables parallel execution, which speeds up computations when working with large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y7inFXIrVWMI"
      },
      "outputs": [],
      "source": [
        "import uproot  # For reading ROOT files efficiently\n",
        "import awkward as ak  # To represent nested data in columnar format\n",
        "import numpy as np  # For numerical calculations such as histogramming\n",
        "import pandas as pd # For dataframes, a format widely used in python\n",
        "import matplotlib.pyplot as plt  # For creating plots and visualizations\n",
        "from matplotlib.ticker import AutoMinorLocator  # For adding minor ticks to plot axes\n",
        "import atlasopenmagic as atom  # Provides access to ATLAS Open Data metadata and streaming URLs\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed # Enables parallel execution for faster processing of large datasets\n",
        "\n",
        "import time  # For timing operations and adding delays if needed\n",
        "# Filter warnings that otherwise appear in output. These are normal in the running of this notebook.\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in sqrt\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in power\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in multiply\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeWGPZrCeWTT"
      },
      "source": [
        "# **Explanation of Key Parameters in the $t\\bar{t}$ Analysis Code**\n",
        "\n",
        "In particle physics analyses, various parameters and constants are essential for accurately processing data and interpreting results. Below is an explanation of the key parameters used in the\n",
        " $t\\bar{t}$ analysis code provided:\n",
        "\n",
        "1.   **Integrated Luminosity (lumi)**\n",
        "\n",
        "  *   Definition: Integrated luminosity is a measure of the total amount of data collected by a particle detector over a certain period. It represents the total number of potential collisions that could have occurred in a particle accelerator and is typically measured in inverse femtobarns fb$^{-1}$.\n",
        "\n",
        "2. **Fraction of Events to Process**\n",
        "\n",
        "  *   Definition: This parameter controls what fraction of the available events in the dataset will be processed by the analysis.\n",
        "\n",
        "\n",
        "3. **MV2c10 b-Tagging Algorithm Discriminant Cut Value Definition**\n",
        "  * The MV2c10 is a b-tagging algorithm used to identify jets originating from b-quarks. The discriminant cut value determines the threshold for tagging a jet as a b-jet.\n",
        "\n",
        "  For further information visit [atlas glossary](https://atlas.cern/glossary)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rMdNPb42cNA5"
      },
      "outputs": [],
      "source": [
        "# Integrated luminosity in inverse picobarns\n",
        "lumi = 36000.\n",
        "\n",
        "# Fraction of events to process\n",
        "fraction = 0.01\n",
        "\n",
        "# MV2c10 b-tagging algorithm discriminant cut value\n",
        "MV2c10_lim = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6CY1t7X1YzH"
      },
      "source": [
        "# **Samples**\n",
        "Samples to process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atom.set_release('2025e-13tev-beta')\n",
        "\n",
        "mc_defs = {\n",
        "    r'ttbar':    {'dids': [601495,410081,410470]},\n",
        "    r'single_top': {'dids': [601355,601487,601627,601628,601631,601761,601762,601763,601764]},\n",
        "    r'diboson':  {'dids': [700488,700489,700490,700491,700492,700493,700495,700496]},\n",
        "}\n",
        "\n",
        "mc_samples   = atom.build_mc_dataset(mc_defs, skim='1LMET30', protocol='https')\n",
        "data_samples_1 = atom.build_data_dataset('1LMET30', name=\"Data\", protocol='https')\n",
        "\n",
        "samples = {**data_samples_1, **mc_samples}\n",
        "\n",
        "variables = [\"mcWeight\", \"ScaleFactor_LepTRIGGER\", \"trigE\", \"trigM\", \"lep_n\", \"lep_pt\",\"ScaleFactor_BTAG\",\"lep_isMediumID\",\n",
        "            \"lep_eta\", \"lep_phi\", \"lep_charge\", \"lep_type\", \"met\", \"met_phi\", \"sum_of_weights\", \"ScaleFactor_FTAG\",\"lep_isLooseIso\",\n",
        "            \"xsec\", \"jet_pt\", \"jet_btag_quantile\", \"jet_n\", \"jet_eta\", \"jet_phi\", \"jet_e\",\"jet_jvt\",\"lep_isTrigMatched\",\n",
        "            \"lep_type\", \"lep_e\", \"eventNumber\", \"ScaleFactor_ELE\", \"ScaleFactor_MUON\",\"ScaleFactor_PILEUP\",\"filteff\", \"kfac\"]"
      ],
      "metadata": {
        "id": "HSl8c7wuCply"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wf_zDJbcTxw"
      },
      "source": [
        "# **Weight in Particle Physics Analysis**\n",
        "\n",
        "In particle physics analysis, such as in the study of the production of top-antitop quark pairs $t\\bar{t}$, the concept of \"weight\" plays a crucial role. Weights are factors applied to events or data points in a dataset to ensure that the results of an analysis accurately reflect the underlying physics being studied. These weights account for various factors, including the efficiencies of detectors, the probability of certain processes occurring, and the corrections needed to match the simulated data with real-world observations.\n",
        "\n",
        "The following function computes the event weights by combining several correction factors, including trigger efficiencies, pileup corrections, b-tagging efficiencies, and cross-sections. The result is a set of weights that accurately reflect the likelihood and significance of each event, ensuring that the final analysis properly accounts for all relevant physical and experimental considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcbBI4BAcTF7"
      },
      "outputs": [],
      "source": [
        "def calc_weight(data):\n",
        "    # Calculate the weight\n",
        "    weight = (data[\"ScaleFactor_PILEUP\"] * data[\"ScaleFactor_LepTRIGGER\"] * data[\"ScaleFactor_BTAG\"] *\n",
        "              data[\"ScaleFactor_ELE\"] * data[\"ScaleFactor_MUON\"] *\n",
        "             (data[\"mcWeight\"] / data[\"sum_of_weights\"]) * (lumi * data[\"xsec\"] * data[\"kfac\"]   * data[\"filteff\"] ) )\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUDeftK2kkx"
      },
      "source": [
        "# **Introduction to Event Selection Cuts in $t\\bar{t}$ Analysis**\n",
        "\n",
        "In particle physics, particularly in the analysis of top-antitop $t\\bar{t}$ quark pair production, the process of event selection is crucial for isolating the signal from the background. The goal is to apply a series of cuts, or selection criteria, that filter out events unlikely to be associated with the $t\\bar{t}$ process, leaving a dataset enriched with signal events. These cuts are based on specific physical properties of the events, such as the presence and characteristics of leptons, missing transverse energy $E^{miss}_{T}$, and jets.\n",
        "\n",
        "Each cut is designed to enhance the signal-to-background ratio, which is essential for making a clear observation of the $t\\bar{t}$ signal. Below is an overview of the key cuts applied in this analysis:\n",
        "\n",
        "1. **Trigger Selection:**\n",
        "  * The first step in the analysis is to ensure that the events under consideration have fired the appropriate triggers.\n",
        "  * Purpose: This cut checks whether either the electron trigger ('trigE') or the muon trigger ('trigM') has been activated. If either trigger fired, the event is considered for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQVmJwTQrpNT"
      },
      "outputs": [],
      "source": [
        "def cut_trig(trigE, trigM):\n",
        "    # Return True if either electron or muon trigger fired\n",
        "    return trigE | trigM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. **Matched Cut:**\n",
        "\n",
        "   *    This cut ensures that the event contains one lepton that is matched with a trigger.\n",
        "\n",
        "   *     Purpose: The cut ensures that one lepton in the event is associated with a trigger, meaning it was responsible for the event being recorded. Applying this cut reduces backgrounds from events where no lepton is properly matched to a trigger, improving the selection efficiency for signal events."
      ],
      "metadata": {
        "id": "e32e64_7ycAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Matched_cut(Matched):\n",
        "    return ak.flatten(Matched)"
      ],
      "metadata": {
        "id": "lLNUcOQEycTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNis73fg37Xx"
      },
      "source": [
        "3. **Single Lepton Requirement:**\n",
        "  * To focus on events characteristic of $t\\bar{t}$ decays, we require exactly one lepton (electron or muon) in the event.\n",
        "  * Purpose: This cut ensures that the event contains precisely one lepton, which is typical in semileptonic $t\\bar{t}$ decays where one top quark decays leptonically, and the other decays hadronically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E7DleQT37y6"
      },
      "outputs": [],
      "source": [
        "def one_lep(lep_n):\n",
        "    # Return True if exactly one lepton in the event\n",
        "    return lep_n == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vmYaGU38WB"
      },
      "source": [
        "4. **Lepton Transverse Momentum ($p_𝑇$) Cut:**\n",
        "  * High transverse momentum leptons are a key feature of $t\\bar{t}$ events.\n",
        "  * Purpose: This cut selects events where the transverse momentum of the lepton is above 30 GeV, ensuring that only significant lepton candidates are considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1P1qI2_4hB2"
      },
      "outputs": [],
      "source": [
        "def cut_lep_pt(lep_pt):\n",
        "    # Return True if lepton pT is above 30 GeV and save a list of the lepton pT in GeV\n",
        "    return lep_pt >= 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWOAlMwZ4hfX"
      },
      "source": [
        "5. **Missing Transverse Energy ($E^{miss}_{T}$) Cut:**\n",
        "\n",
        "  * $t\\bar{t}$ events often have significant missing transverse energy due to the neutrinos produced in the decay.\n",
        "  * Purpose: This cut selects events where the missing transverse energy is greater than 30 GeV, helping to reduce background from processes without significant $E^{miss}_{T}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j0o3fdE4jNw"
      },
      "outputs": [],
      "source": [
        "def cut_met_et(met):\n",
        "    # Return True if missing ET is above 30 GeV and save a list of the missing ET in GeV\n",
        "    return met >= 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg7hrt2x4jvN"
      },
      "source": [
        "6. **W Boson Transverse Mass  ($M^{W}_{T}$) Cut:**\n",
        "  * The transverse mass of the W boson, reconstructed from the lepton transverse momentum and $E^{miss}_{T}$.\n",
        "  * Purpose: This cut ensures that the transverse mass of the W boson is above 30 GeV, further reducing background contamination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny7IW8CP4oJF"
      },
      "outputs": [],
      "source": [
        "def cut_W_mt(lep_pt, lep_phi, met_et, met_phi):\n",
        "    # Calculate W transverse mass and apply cut\n",
        "    Wmt = 2 * lep_pt * met_et * (1 - np.cos(lep_phi - met_phi))\n",
        "    cut = ak.where(Wmt > 0, np.sqrt(Wmt), 0) >= 30\n",
        "    return cut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D24AJBj4ouS"
      },
      "source": [
        "7. **Jet and b-Jet Multiplicity Cuts:**\n",
        "  * $t\\bar{t}$ events typically feature multiple jets, including those tagged as b-jets.\n",
        "  * Purpose: This cut requires at least four jets, with at least two of them identified as b-jets. This helps isolate the $t\\bar{t}$ signal from other processes that produce fewer jets or no b-jets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcsYwgHf4pGH"
      },
      "outputs": [],
      "source": [
        "def cut_Njet_and_Nbjet(jet_pt, jet_MV2c10):\n",
        "    # Count jets above pT threshold\n",
        "    num_pt = ak.sum(jet_pt >= 30, axis=1) >= 4\n",
        "\n",
        "    # Count b-tagged jets\n",
        "    num_btag = ak.sum(jet_MV2c10 >= MV2c10_lim, axis=1) >= 2\n",
        "\n",
        "    # Require at least 4 jets and 2 b-tagged jets\n",
        "    return (num_pt) & (num_btag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCJSrLwI81y_"
      },
      "source": [
        "# **Introduction to Mass Reconstruction in $t\\bar{t}$ Analysis**\n",
        "\n",
        "In the study of top-antitop quark pair production ($t\\bar{t}$), accurately reconstructing the masses of the decayed particles is essential for identifying the signal and distinguishing it from background processes. The top quark, the heaviest known elementary particle, typically decays into a W boson and a b-quark. In semileptonic $t\\bar{t}$ decays, one W boson decays into a lepton and a neutrino, while the other W boson decays hadronically into jets.\n",
        "\n",
        "To study these decays, we reconstruct the masses of the hadronically decaying top quark and the semileptonic top quark using the kinematic information of the detected particles. This process involves complex calculations that take into account the momentum, energy, and spatial distributions of the leptons, neutrinos, and jets involved in the decay.\n",
        "\n",
        "The following function, mtop, performs these calculations, reconstructing the mass of the top quarks in both their hadronic and semileptonic decay channels:\n",
        "\n",
        "**1. Leptonic Top Quark Mass ($M_{lvb}$)**\n",
        "* Purpose: The leptonic top quark mass is reconstructed using the lepton, the b-jet closest to the lepton, and the missing transverse energy (which is attributed to the neutrino).\n",
        "\n",
        "* Steps:\n",
        "  * Kinematic Conversions: Convert the energy and momentum of the lepton and neutrino from the detector units to GeV.\n",
        "\n",
        "  * Quadratic Solutions: Calculate the neutrino's longitudinal momentum ($p_{z}$) by solving a quadratic equation derived from the W boson mass constraint.\n",
        "\n",
        "  * Reconstruction: Combine the lepton, neutrino, and b-jet kinematics to compute the mass of the leptonic top quark for both solutions of $p_{z}$.\n",
        "2. Hadronic Top Quark Mass ($M_{jjj}$)\n",
        "\n",
        "* Purpose: The hadronic top quark mass is reconstructed using the b-jet farthest from the lepton and two other jets that form a pair with a mass close to that of the W boson.\n",
        "* Steps:\n",
        "  * Jet Pairing: Identify and pair jets that are not b-tagged, and calculate their invariant mass to find the pair closest to the W boson mass.\n",
        "  * Reconstruction: Combine the selected jet pair with the farthest b-jet to compute the mass of the hadronic top quark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNP7dAe39Es"
      },
      "outputs": [],
      "source": [
        "def mtop(jet_pt, jet_E, jet_eta, jet_phi, jet_MV2c10, lep_pt, lep_eta, lep_phi, lep_E, met_et, met_phi):\n",
        "    # This function calculates the mass of semileptonic decaying top quark and hadronic decay top quark\n",
        "\n",
        "    # Convert lepton kinematics\n",
        "    lep_px = lep_pt * np.cos(lep_phi)\n",
        "    lep_py = lep_pt * np.sin(lep_phi)\n",
        "    lep_pz = lep_pt / np.tan(2.0 * np.arctan( np.exp( -lep_eta ) ) )\n",
        "\n",
        "    # Convert neutrino kinematics\n",
        "    met_px = met_et * np.cos(met_phi)\n",
        "    met_py = met_et * np.sin(met_phi)\n",
        "\n",
        "    # Calculate neutrino pz solutions/\n",
        "    # Calculate coefficient 'a' of the quadratic equation\n",
        "    a = 4 * (lep_E**2 - lep_pz**2)\n",
        "\n",
        "    # Calculate coefficient 'b' of the quadratic equation, 6464.16 is (80.4 GeV)^2, which is the W boson mass squared\n",
        "    b = -4 * (6464.16 + ((lep_px + met_px)**2) +\n",
        "              ((lep_py + met_py)**2) - (lep_E**2) - (met_et**2) + (lep_pz**2)) * lep_pz\n",
        "\n",
        "    # Calculate coefficient 'c'\n",
        "    c = 4 * (lep_E**2) * (met_et**2) - (6464.16 + ((lep_px + met_px)**2) +\n",
        "               ((lep_py + met_py)**2) - (lep_E**2) - (met_et**2) + (lep_pz**2))**2\n",
        "\n",
        "    # Calculate the discriminant\n",
        "    Delta = b**2 - 4 * a * c\n",
        "\n",
        "    # Calculate the two solutions for met_pz using the quadratic formula\n",
        "    # If Delta is negative, set the solution to NaN\n",
        "    met_pz1 = ak.where(Delta >= 0, (-b + np.sqrt(Delta)) / ( 2 * a), np.nan)\n",
        "    met_pz2 = ak.where(Delta >= 0, (-b - np.sqrt(Delta)) / ( 2 * a), np.nan)\n",
        "\n",
        "    # Calculate the two solutions for met_E\n",
        "    met_E_1 = np.sqrt(met_px**2 + met_py**2 + met_pz1**2)\n",
        "    met_E_2 = np.sqrt(met_px**2 + met_py**2 + met_pz2**2)\n",
        "\n",
        "    # Convert jet kinematics to GeV\n",
        "    px = jet_pt * np.cos(jet_phi)\n",
        "    py = jet_pt * np.sin(jet_phi)\n",
        "    pz = jet_pt / np.tan(2.0 * np.arctan(np.exp(-jet_eta)))\n",
        "\n",
        "    # Identify b-tagged jets\n",
        "    b_tagged = jet_MV2c10 >= MV2c10_lim\n",
        "\n",
        "    # Separate b-tagged and non-b-tagged jets\n",
        "    b_tagged_px, b_tagged_py, b_tagged_pz, b_tagged_E = px[b_tagged], py[b_tagged], pz[b_tagged], jet_E[b_tagged]\n",
        "\n",
        "    b_tagged_eta, b_tagged_phi = jet_eta[b_tagged], jet_phi[b_tagged]\n",
        "\n",
        "    non_b_px, non_b_py, non_b_pz, non_b_E = px[~b_tagged], py[~b_tagged], pz[~b_tagged], jet_E[~b_tagged]\n",
        "\n",
        "\n",
        "    # Calculate delta R between lepton and b-tagged jets\n",
        "    lep_eta_broadcasted, _ = ak.broadcast_arrays(ak.flatten(lep_eta), b_tagged_eta)\n",
        "    lep_phi_broadcasted, _ = ak.broadcast_arrays(ak.flatten(lep_phi), b_tagged_phi)\n",
        "    temp_dR = (lep_eta_broadcasted - b_tagged_eta)**2 + (lep_phi_broadcasted - b_tagged_phi)**2\n",
        "    dR = ak.where(temp_dR >=0 ,np.sqrt(temp_dR),np.nan)\n",
        "\n",
        "    # Find indices of closest and farthest b-tagged jets to the lepton\n",
        "    max_dR_indices = ak.singletons(ak.argmax(dR, axis=1))\n",
        "    min_dR_indices = ak.singletons(ak.argmin(dR, axis=1))\n",
        "\n",
        "    # Extract kinematics of closest b-tagged jets\n",
        "    closest_b_jet_E, closest_b_jet_px = b_tagged_E[min_dR_indices], b_tagged_px[min_dR_indices]\n",
        "    closest_b_jet_py, closest_b_jet_pz = b_tagged_py[min_dR_indices], b_tagged_pz[min_dR_indices]\n",
        "\n",
        "    # Extract kinematics of farthest b-tagged jets\n",
        "    farthest_b_jet_E, farthest_b_jet_px = b_tagged_E[max_dR_indices], b_tagged_px[max_dR_indices]\n",
        "    farthest_b_jet_py, farthest_b_jet_pz = b_tagged_py[max_dR_indices], b_tagged_pz[max_dR_indices]\n",
        "\n",
        "    # Helper function to create combinations of jets\n",
        "    def combo(list1):\n",
        "        jets_pairs = ak.combinations(list1, 2, fields=['List1', 'List2'])\n",
        "        sum_List = jets_pairs['List1'] + jets_pairs['List2']\n",
        "        return sum_List\n",
        "\n",
        "    # Create combinations of non-b-tagged jets\n",
        "    com_non_b_px, com_non_b_py = combo(non_b_px), combo(non_b_py)\n",
        "    com_non_b_pz, com_non_b_E = combo(non_b_pz), combo(non_b_E)\n",
        "\n",
        "    # Calculate mass difference from W boson mass\n",
        "    W_mass = np.sqrt(com_non_b_E**2 - (com_non_b_px**2 + com_non_b_py**2 + com_non_b_pz**2))\n",
        "    com_DM_W = 80.4 - W_mass\n",
        "    abs_diff = np.abs(com_DM_W)\n",
        "\n",
        "    # Select jet pairs close to W mass\n",
        "    check_abs = abs_diff <= 20\n",
        "    check_abs_array = ak.singletons(ak.any(check_abs, axis=1))\n",
        "\n",
        "    # Find best jet pair\n",
        "    min_diff_indices = ak.argmin(abs_diff, axis=1)\n",
        "    min_indices = ak.singletons(min_diff_indices)\n",
        "\n",
        "    # Extract kinematics of selected jet pair\n",
        "    sel_com_jet_E = com_non_b_E[min_indices]\n",
        "    sel_com_jet_px, sel_com_jet_py = com_non_b_px[min_indices], com_non_b_py[min_indices]\n",
        "    sel_com_jet_pz = com_non_b_pz[min_indices]\n",
        "\n",
        "    # Handle cases where no suitable jet pair is found\n",
        "    sel_com_jet_E = ak.without_parameters(ak.where(ak.num(sel_com_jet_E, axis=1) == 0, np.nan, sel_com_jet_E))\n",
        "    sel_com_jet_px = ak.without_parameters(ak.where(ak.num(sel_com_jet_px, axis=1) == 0, np.nan, sel_com_jet_px))\n",
        "    sel_com_jet_py = ak.without_parameters(ak.where(ak.num(sel_com_jet_py, axis=1) == 0, np.nan, sel_com_jet_py))\n",
        "    sel_com_jet_pz = ak.without_parameters(ak.where(ak.num(sel_com_jet_pz, axis=1) == 0, np.nan, sel_com_jet_pz))\n",
        "\n",
        "    # Calculate hadronic top\n",
        "    temp_m_jjj = ((sel_com_jet_E + farthest_b_jet_E)**2 - (sel_com_jet_px + farthest_b_jet_px)**2 -\n",
        "                  (sel_com_jet_py + farthest_b_jet_py)**2 - (sel_com_jet_pz + farthest_b_jet_pz)**2)\n",
        "\n",
        "    m_jjj = ak.where(temp_m_jjj>=0, np.sqrt(temp_m_jjj),np.nan)\n",
        "\n",
        "    # Apply W mass constraint\n",
        "    m_jjj = ak.where(check_abs_array, m_jjj, np.nan).tolist()\n",
        "\n",
        "    # Calculate leptonic top mass for both neutrino pz solutions\n",
        "    m_lvb_1 = np.sqrt((lep_E + met_E_1 + closest_b_jet_E)**2 - ( (lep_px + met_px + closest_b_jet_px)**2 +\n",
        "     (lep_py + met_py + closest_b_jet_py)**2 + (lep_pz + met_pz1 + closest_b_jet_pz)**2 ))\n",
        "\n",
        "    m_lvb_2 = np.sqrt((lep_E + met_E_2 + closest_b_jet_E)**2 - ( (lep_px + met_px + closest_b_jet_px)**2 +\n",
        "     (lep_py + met_py + closest_b_jet_py)**2 + (lep_pz + met_pz2 + closest_b_jet_pz)**2 ))\n",
        "\n",
        "    # Flatten and return results\n",
        "    m_jjj, m_lvb_1, m_lvb_2 = ak.flatten(m_jjj), ak.flatten(m_lvb_1), ak.flatten(m_lvb_2)\n",
        "    return [m_jjj, m_lvb_1, m_lvb_2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Event Processing Function**\n",
        "\n",
        "The process_file function processes ROOT files by:\n",
        "\n",
        "1. **Loading data**: Opens the TTree and reads specified variables in chunks\n",
        "2. **Applying selection cuts**: Implements a sequential series of cuts\n",
        "3. **Computing derived quantities**: Calculates invariant mass and event weights\n",
        "\n",
        "The function processes data in chunks to manage memory efficiently and returns a flattened array of all events that pass the selection criteria."
      ],
      "metadata": {
        "id": "ioVFvpxp1RAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path, sample_name,loop):\n",
        "    # Open the 'analysis' TTree from the ROOT file\n",
        "    tree = uproot.open(file_path + \":analysis\")\n",
        "    sample_data = []\n",
        "\n",
        "    for data in tree.iterate(variables, library=\"ak\", entry_start=int(tree.num_entries * fraction * loop),\n",
        "                             entry_stop=int(tree.num_entries * fraction * (loop+1))):\n",
        "\n",
        "            data = data[cut_trig(data.trigE,data.trigM)]\n",
        "            data = data[one_lep(data.lep_n)]\n",
        "            data = data[Matched_cut(data.lep_isTrigMatched)]\n",
        "            data = data[ak.flatten(cut_lep_pt(data.lep_pt))]\n",
        "            data = data[cut_met_et(data.met)]\n",
        "            data = data[ak.flatten(cut_W_mt(data.lep_pt, data.lep_phi, data.met, data.met_phi))]\n",
        "            data = data[cut_Njet_and_Nbjet(data.jet_pt,data.jet_btag_quantile)]\n",
        "\n",
        "            mtop_data = mtop(data.jet_pt ,data.jet_e ,data.jet_eta ,data.jet_phi ,data.jet_btag_quantile ,\n",
        "                          data.lep_pt, data.lep_eta ,data.lep_phi, data.lep_e, data.met, data.met_phi)\n",
        "\n",
        "            data['mtop'] = mtop_data[0]\n",
        "\n",
        "            data['mtop_1'] = mtop_data[1]\n",
        "\n",
        "            data['mtop_2'] = mtop_data[2]\n",
        "\n",
        "            if 'Data' not in sample_name:\n",
        "                data['Weight'] = calc_weight(data)\n",
        "            else:\n",
        "                data['Weight'] = ak.ones_like(data['met'])\n",
        "\n",
        "            sample_data.append(data)\n",
        "\n",
        "    # Concatenate all data from the current file into a single array\n",
        "    return ak.concatenate(sample_data, axis=0)"
      ],
      "metadata": {
        "id": "Onz5XOmA1RU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parallel Analysis Function**\n",
        "\n",
        "The parallel_analysis function processes a file in parallel using multiple processes to analyze data for a given sample. It submits four tasks to process the file using the ProcessPoolExecutor. Each task calls the process_file function. The results from successful tasks are collected and combined into a single Awkward Array."
      ],
      "metadata": {
        "id": "aDklugLu1Xw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_analysis(file_path, sample_name):\n",
        "    # Parallel processing\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        futures = {executor.submit(process_file, file_path, sample_name, 0),\n",
        "                  executor.submit(process_file, file_path, sample_name, 1),\n",
        "                  executor.submit(process_file, file_path, sample_name, 2),\n",
        "                  executor.submit(process_file, file_path, sample_name, 3)}\n",
        "\n",
        "        results = []\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                results.append(future.result())\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    # Combine results into an Awkward Array\n",
        "    combined_array = ak.concatenate(results, axis=0) if results else ak.Array([])\n",
        "\n",
        "    return combined_array"
      ],
      "metadata": {
        "id": "5VFUGY9W1YGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Analysis Loop**\n",
        "\n",
        "This code initiates the full analysis by iterating over all data and MC samples defined in the `samples` dictionary. For each sample, it processes the associated ROOT files using the `parallel_analysis` function and stores the resulting data in the `data_all` dictionary. The processed outputs are combined into a single Awkward Array per sample."
      ],
      "metadata": {
        "id": "-Fb28ZvZ5wlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_all = time.time()  # Define start time\n",
        "data_all = {}  # Dictionary to store results for each sample\n",
        "fraction = 0.1 # Lower this vaule for less running time\n",
        "print(\"The analysis has started\")\n",
        "\n",
        "for s in samples:\n",
        "    frames = []\n",
        "    print(\"processing the \",s,\" samples\")\n",
        "\n",
        "    # Loop over ROOT files associated with the current sample\n",
        "    for val in samples[s]['list']:\n",
        "\n",
        "        DF = process_file(val, s,0)\n",
        "        #DF = parallel_analysis(val, s)\n",
        "        frames.append(DF) # Collect the results\n",
        "\n",
        "    # Store the frames for this sample\n",
        "    data_all[s] = ak.concatenate(frames, axis=0)\n",
        "\n",
        "end_all = time.time()\n",
        "print(f\"\\nTotal time taken to process all samples: {round((end_all - start_all) / 60, 1)} minutes\")"
      ],
      "metadata": {
        "id": "W4doHy--ERM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combining Mass and Weight Lists**\n",
        "In the context of $t\\bar{t}$ analysis, when reconstructing the mass of the top quark from the leptonic decay, we often encounter two possible solutions for the neutrino's momentum due to the nature of the quadratic equation involved in the reconstruction. These two solutions lead to two potential values for the top quark mass. To account for both possibilities in the analysis, we combine these two mass values into a single list. However, since both solutions are considered equally probable, the weight for each event must be divided by 2 to avoid double counting"
      ],
      "metadata": {
        "id": "3hX0Z1jW5-RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_all_2 = {\n",
        "    \"Data\": {\n",
        "        \"Weight\": ak.Array([1/2] * 2 * len(data_all[\"Data\"][\"mtop_2\"])),\n",
        "        \"mtop\": ak.flatten([data_all[\"Data\"][\"mtop_1\"], data_all[\"Data\"][\"mtop_2\"]])\n",
        "    },\n",
        "    \"ttbar\": {\n",
        "        \"Weight\": ak.flatten([data_all[\"ttbar\"][\"Weight\"], data_all[\"ttbar\"][\"Weight\"]]) / 2,\n",
        "        \"mtop\": ak.flatten([data_all[\"ttbar\"][\"mtop_1\"], data_all[\"ttbar\"][\"mtop_2\"]])\n",
        "    },\n",
        "    \"diboson\": {\n",
        "        \"Weight\": ak.flatten([data_all[\"diboson\"][\"Weight\"], data_all[\"diboson\"][\"Weight\"]]) / 2,\n",
        "        \"mtop\": ak.flatten([data_all[\"diboson\"][\"mtop_1\"], data_all[\"diboson\"][\"mtop_2\"]])\n",
        "    },\n",
        "    \"single_top\": {\n",
        "        \"Weight\": ak.flatten([data_all[\"single_top\"][\"Weight\"], data_all[\"single_top\"][\"Weight\"]]) / 2,\n",
        "        \"mtop\": ak.flatten([data_all[\"single_top\"][\"mtop_1\"], data_all[\"single_top\"][\"mtop_2\"]])\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "G4brTrq9EMt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting**\n",
        "\n",
        "Define function to plot the data"
      ],
      "metadata": {
        "id": "FXuDcJzsENA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(data, x_axis_label):\n",
        "    # Define plot parameters\n",
        "    xmin, xmax, step_size = 100, 250, 3\n",
        "\n",
        "    # Define MC data sets and their properties\n",
        "    datasets = [\n",
        "        {'data': data[\"diboson\"]['mtop'], 'weights': data[\"diboson\"]['Weight'], 'color': 'blue', 'label': r'Diboson'},\n",
        "        {'data': data[\"single_top\"]['mtop'], 'weights': data[\"single_top\"]['Weight'], 'color': 'cyan', 'label': r'Single top'},\n",
        "        {'data': data[\"ttbar\"]['mtop'], 'weights': data[\"ttbar\"]['Weight'], 'color': 'orange', 'label': r'$t\\bar{t}$'}\n",
        "    ]\n",
        "\n",
        "    # Create bin edges and centers\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = np.arange(xmin + step_size/2, xmax + step_size/2, step_size)\n",
        "\n",
        "    # Histogram the data and weights for the data\n",
        "    data_x, _ = np.histogram(ak.to_numpy(data[\"Data\"]['mtop']), bins=bin_edges, weights=data[\"Data\"]['Weight'])\n",
        "\n",
        "    data_x_errors = np.sqrt(data_x)  # statistical error on the data\n",
        "\n",
        "    # Create main plot and residual subplot\n",
        "    fig, (main_axes, residual_axes) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    # Plot data with error bars\n",
        "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
        "                       fmt='ko', label='Data')\n",
        "\n",
        "    # Plot the Monte Carlo bars\n",
        "    mc_heights = main_axes.hist([d['data'] for d in datasets], bins=bin_edges,\n",
        "                                weights=[d['weights'] for d in datasets], stacked=True,\n",
        "                                color=[d['color'] for d in datasets], label=[d['label'] for d in datasets])\n",
        "\n",
        "    mc_x_tot = (mc_heights[0][2])  # Stacked background MC y-axis value\n",
        "\n",
        "    # Calculate MC statistical uncertainty: sqrt(sum w^2)\n",
        "    mc_x_err = np.sqrt(np.histogram(np.hstack([d['data'] for d in datasets]), bins=bin_edges, weights=np.hstack([d['weights'] for d in datasets])**2)[0])\n",
        "\n",
        "    # Plot the statistical uncertainty\n",
        "    main_axes.bar(bin_centres, 2*mc_x_err, alpha=0.5, bottom=mc_x_tot-mc_x_err, color='none', hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
        "\n",
        "    # Set up main axes\n",
        "    main_axes.set_xlim(left=xmin, right=xmax)\n",
        "    main_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    main_axes.set_ylabel('Events', y=1, horizontalalignment='right')\n",
        "    main_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.set_ylim(0)\n",
        "\n",
        "    # Add text to the plot\n",
        "    main_axes.text(0.05, 0.93, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13)\n",
        "    main_axes.text(0.05, 0.88, 'for education', transform=main_axes.transAxes, style='italic', fontsize=8)\n",
        "    main_axes.text(0.05, 0.82, r'$\\sqrt{s}$=13 TeV, 36 fb$^{-1}$', transform=main_axes.transAxes)\n",
        "    main_axes.text(0.05, 0.76, r'$t\\bar{t} \\rightarrow \\ell v b\\bar{b} q\\bar{q}$', transform=main_axes.transAxes)\n",
        "\n",
        "    main_axes.legend(frameon=False)\n",
        "\n",
        "    # Calculate and plot residuals\n",
        "    ratio = data_x / (mc_heights[0][2])\n",
        "    residual_axes.errorbar(bin_centres, ratio, yerr=ratio*data_x_errors/data_x, fmt='ko')\n",
        "    residual_axes.axhline(1, color='r', linestyle='--')\n",
        "    residual_axes.set_ylim(0.5,1.5)\n",
        "    residual_axes.set_xlabel(x_axis_label, fontsize=13, x=1, horizontalalignment='right')\n",
        "    residual_axes.set_ylabel('Ratio (Data/MC)')\n",
        "    residual_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "\n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace=0.05)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HtcEdP2ZEJCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the function to plot the data"
      ],
      "metadata": {
        "id": "djnBA3BCEIxt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA0ew410fDHr"
      },
      "outputs": [],
      "source": [
        "# Plot the trijet mass distribution (m_jjj)\n",
        "plot_data(data_all,r\"$\\mathrm{m_{jjj}} \\ [GeV]$\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the lepton-neutrino-b-jet mass distribution (m_lvb)\n",
        "plot_data(data_all_2,r\"$\\mathrm{m_{lvb}} \\ [GeV]$\")"
      ],
      "metadata": {
        "id": "xeZYkYSlzCyP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoPapadopoulos/ttbar_analysis/blob/main/ttbar_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9CDWygzhxRE"
      },
      "source": [
        "#**How to Rediscover the Top-Antitop $t\\bar{t}$ Quark Pair Production Yourself!**\n",
        "\n",
        "This notebook uses ATLAS Open Data to guide you through the steps needed to rediscover the production of top-antitop quark pairs $t\\bar{t}$ in high-energy proton-proton collisions at the Large Hadron Collider (LHC).\n",
        "\n",
        "ATLAS Open Data provides open access to proton-proton collision data collected by the ATLAS experiment at the LHC. These datasets are made available for educational purposes, making them ideal for high-school, undergraduate, and postgraduate students interested in learning about particle physics.\n",
        "\n",
        "# **What Are Notebooks?**\n",
        "\n",
        "Notebooks are interactive web applications that allow you to create and share documents that contain:\n",
        "\n",
        "1. **Live code:** Write and execute code in real-time, making adjustments as you go.\n",
        "\n",
        "2. **Visualizations:** Create plots, histograms, and other graphical representations of your data to better understand the underlying physics.\n",
        "\n",
        "3. **Narrative text:** Include explanations, descriptions, and commentary to guide yourself or others through the analysis.\n",
        "\n",
        "\n",
        "# **The Goal: Rediscovering $t\\bar{t}$ Production**\n",
        "\n",
        "By following this notebook, you will perform a $t\\bar{t}$ analysis, aiming to identify and study the production of top-antitop quark pairs. The process involves applying a series of selection cuts and analysis techniques to increase the ratio of signal (events where $t\\bar{t}$ pairs are produced) to background (other processes that can mimic the signal).\n",
        "\n",
        "A key part of this analysis is focusing on the semileptonic decay of the $t\\bar{t}$ pair. In this decay mode, one top quark decays into a W boson and a b-quark, with the W boson further decaying into a charged lepton and a neutrino, while the other top quark decays hadronically (into jets).This can be represented as:\n",
        "* $ t\\bar{t} \\rightarrow lvb\\bar{b}q\\bar{q}$\n",
        "\n",
        "where:\n",
        "\n",
        "* $l$ is a lepton,\n",
        "* $v$ is a neutrino,\n",
        "* $b$ is a b-tagged jet,\n",
        "* $q$ is a jet.\n",
        "\n",
        "\n",
        "**Contents:**\n",
        "* Running a Jupyter notebook\n",
        "* To setup\n",
        "* Explanation of Key Parameters in the $t\\bar{t}$ Analysis Code\n",
        "* Samples\n",
        "* Weight in Particle Physics Analysis\n",
        "* Introduction to Event Selection Cuts in $t\\bar{t}$ Analysis\n",
        "* Introduction to Mass Reconstruction in $t\\bar{t}$ Analysis\n",
        "* Data Processing and Event Selection Function\n",
        "* Data Retrieval in $t\\bar{t}$ Analysis\n",
        "* Data Processing\n",
        "* Data Aggregation with Loops\n",
        "* Combining Mass and Weight Lists\n",
        "* Plotting\n",
        "\n",
        "\n",
        "# **Running a Jupyter notebook**\n",
        "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
        "\n",
        "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
        "\n",
        "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "63X2lXpgywvZ"
      },
      "outputs": [],
      "source": [
        "!pip install uproot\n",
        "!pip install fsspec-xrootd\n",
        "!pip install --no-build-isolation XRootD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f477h5dJ-SFM"
      },
      "source": [
        "# **To setup**\n",
        "Cell -> Run All Below\n",
        "\n",
        "to be done every time you re-open this notebook\n",
        "\n",
        "We're going to be using a number of tools to help us:\n",
        "\n",
        "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
        "* awkward: lets us use efficiently the nested data in columnar format\n",
        "* pandas: lets us store data as dataframes, a format widely used in python\n",
        "* numpy: provides numerical calculations such as histogramming\n",
        "* matplotlib: common tool for making plots, figures, images, visualisations\n",
        "* concurrent.futures: Enables parallel execution, which speeds up computations when working with large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7inFXIrVWMI"
      },
      "outputs": [],
      "source": [
        "import uproot  # For reading ROOT files efficiently\n",
        "import awkward as ak  # To represent nested data in columnar format\n",
        "import numpy as np  # For numerical calculations such as histogramming\n",
        "import pandas as pd # For dataframes, a format widely used in python\n",
        "import matplotlib.pyplot as plt  # For creating plots and visualizations\n",
        "from matplotlib.ticker import AutoMinorLocator  # For adding minor ticks to plot axes\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed # Enables parallel execution for faster processing of large datasets\n",
        "\n",
        "import time  # For timing operations and adding delays if needed\n",
        "# Filter warnings that otherwise appear in output. These are normal in the running of this notebook.\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in sqrt\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in power\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in multiply\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeWGPZrCeWTT"
      },
      "source": [
        "# **Explanation of Key Parameters in the $t\\bar{t}$ Analysis Code**\n",
        "\n",
        "In particle physics analyses, various parameters and constants are essential for accurately processing data and interpreting results. Below is an explanation of the key parameters used in the\n",
        " $t\\bar{t}$ analysis code provided:\n",
        "\n",
        "1.   **Integrated Luminosity (lumi)**\n",
        "\n",
        "  *   Definition: Integrated luminosity is a measure of the total amount of data collected by a particle detector over a certain period. It represents the total number of potential collisions that could have occurred in a particle accelerator and is typically measured in inverse femtobarns fb$^{-1}$.\n",
        "\n",
        "2. **Fraction of Events to Process**\n",
        "\n",
        "  *   Definition: This parameter controls what fraction of the available events in the dataset will be processed by the analysis.\n",
        "\n",
        "\n",
        "3. **MV2c10 b-Tagging Algorithm Discriminant Cut Value Definition**\n",
        "  * The MV2c10 is a b-tagging algorithm used to identify jets originating from b-quarks. The discriminant cut value determines the threshold for tagging a jet as a b-jet.\n",
        "\n",
        "  For further information visit [atlas glossary](https://atlas.cern/glossary)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMdNPb42cNA5"
      },
      "outputs": [],
      "source": [
        "# Integrated luminosity in inverse picobarns\n",
        "lumi = 36000.\n",
        "\n",
        "# Fraction of events to process\n",
        "fraction = 0.01\n",
        "\n",
        "# MV2c10 b-tagging algorithm discriminant cut value\n",
        "MV2c10_lim = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6CY1t7X1YzH"
      },
      "source": [
        "# **Samples**\n",
        "Samples to process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7G4qeaGy9p1"
      },
      "outputs": [],
      "source": [
        "# Dictionary of samples to be processed\n",
        "samples = {\n",
        "\n",
        "    'data': {'list' : [\"data15_periodD\",\"data15_periodE\",\"data15_periodF\",\"data15_periodG\",\n",
        "                       \"data15_periodH\",\"data15_periodJ\",\"data16_periodA\",\"data16_periodB\",\n",
        "                       \"data16_periodC\",\"data16_periodD\",\"data16_periodE\",\"data16_periodF\",\n",
        "                       \"data16_periodG\",\"data16_periodI\",\"data16_periodK\",\"data16_periodL\"],},\n",
        "\n",
        "    'ttbar' : {  'list' : ['mc_601495.PhPy8EG_A14_ttbar_pThard1_allhad',\n",
        "                           'mc_410081.MadGraphPythia8EvtGen_A14NNPDF23_ttbarWW',\n",
        "                           'mc_410470.PhPy8EG_A14_ttbar_hdamp258p75_nonallhad'\n",
        "                        ],},\n",
        "    'single_top' : {  'list' : ['mc_601355.PhPy8EG_tW_dyn_DR_incl_top',\n",
        "                                'mc_601487.PhPy8EG_A14_tchan_pThard1_lep_antitop',\n",
        "                                'mc_601627.PhPy8EG_tW_DS_dyn_incl_antitop',\n",
        "                                'mc_601628.PhPy8EG_tW_DS_dyn_dil_top',\n",
        "                                'mc_601631.PhPy8EG_tW_DS_dyn_incl_top',\n",
        "                                'mc_601761.PhPy8EG_tW_dyn_DR_pThard1_incl_top',\n",
        "                                'mc_601762.PhPy8EG_tW_dyn_DR_pThard1_incl_antitop',\n",
        "                                'mc_601763.PhPy8EG_tW_dyn_DR_pThard1_dil_top',\n",
        "                                'mc_601764.PhPy8EG_tW_dyn_DR_pThard1_dil_antitop',\n",
        "                        ],},\n",
        "\n",
        "    'diboson' : {  'list' : ['mc_700488.Sh_2211_WlvWqq',\n",
        "                             'mc_700489.Sh_2211_WlvZqq',\n",
        "                             'mc_700490.Sh_2211_WlvZbb',\n",
        "                             'mc_700491.Sh_2211_WqqZvv',\n",
        "                             'mc_700492.Sh_2211_WqqZll',\n",
        "                             'mc_700493.Sh_2211_ZqqZll',\n",
        "                             'mc_700495.Sh_2211_ZqqZvv',\n",
        "                             'mc_700496.Sh_2211_ZbbZvv'\n",
        "                        ],}\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wf_zDJbcTxw"
      },
      "source": [
        "# **Weight in Particle Physics Analysis**\n",
        "\n",
        "In particle physics analysis, such as in the study of the production of top-antitop quark pairs $t\\bar{t}$, the concept of \"weight\" plays a crucial role. Weights are factors applied to events or data points in a dataset to ensure that the results of an analysis accurately reflect the underlying physics being studied. These weights account for various factors, including the efficiencies of detectors, the probability of certain processes occurring, and the corrections needed to match the simulated data with real-world observations.\n",
        "\n",
        "The following function computes the event weights by combining several correction factors, including trigger efficiencies, pileup corrections, b-tagging efficiencies, and cross-sections. The result is a set of weights that accurately reflect the likelihood and significance of each event, ensuring that the final analysis properly accounts for all relevant physical and experimental considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcbBI4BAcTF7"
      },
      "outputs": [],
      "source": [
        "def calc_weight(data):\n",
        "    # Calculate the weight\n",
        "    weight = (data[\"ScaleFactor_PILEUP\"] * data[\"ScaleFactor_LepTRIGGER\"] * data[\"ScaleFactor_BTAG\"] *\n",
        "              data[\"ScaleFactor_ELE\"] * data[\"ScaleFactor_MUON\"] *\n",
        "             (data[\"mcWeight\"] / data[\"sum_of_weights\"]) * (lumi * data[\"xsec\"] * data[\"kfac\"]   * data[\"filteff\"] ) )\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUDeftK2kkx"
      },
      "source": [
        "# **Introduction to Event Selection Cuts in $t\\bar{t}$ Analysis**\n",
        "\n",
        "In particle physics, particularly in the analysis of top-antitop $t\\bar{t}$ quark pair production, the process of event selection is crucial for isolating the signal from the background. The goal is to apply a series of cuts, or selection criteria, that filter out events unlikely to be associated with the $t\\bar{t}$ process, leaving a dataset enriched with signal events. These cuts are based on specific physical properties of the events, such as the presence and characteristics of leptons, missing transverse energy $E^{miss}_{T}$, and jets.\n",
        "\n",
        "Each cut is designed to enhance the signal-to-background ratio, which is essential for making a clear observation of the $t\\bar{t}$ signal. Below is an overview of the key cuts applied in this analysis:\n",
        "\n",
        "1. **Trigger Selection:**\n",
        "  * The first step in the analysis is to ensure that the events under consideration have fired the appropriate triggers.\n",
        "  * Purpose: This cut checks whether either the electron trigger ('trigE') or the muon trigger ('trigM') has been activated. If either trigger fired, the event is considered for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQVmJwTQrpNT"
      },
      "outputs": [],
      "source": [
        "def cut_trig(trigE, trigM):\n",
        "    # Return True if either electron or muon trigger fired\n",
        "    return trigE | trigM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. **Matched Cut:**\n",
        "\n",
        "   *    This cut ensures that the event contains one lepton that is matched with a trigger.\n",
        "\n",
        "   *     Purpose: The cut ensures that one lepton in the event is associated with a trigger, meaning it was responsible for the event being recorded. Applying this cut reduces backgrounds from events where no lepton is properly matched to a trigger, improving the selection efficiency for signal events."
      ],
      "metadata": {
        "id": "e32e64_7ycAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Matched_cut(Matched):\n",
        "    return ak.flatten(Matched)"
      ],
      "metadata": {
        "id": "lLNUcOQEycTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNis73fg37Xx"
      },
      "source": [
        "3. **Single Lepton Requirement:**\n",
        "  * To focus on events characteristic of $t\\bar{t}$ decays, we require exactly one lepton (electron or muon) in the event.\n",
        "  * Purpose: This cut ensures that the event contains precisely one lepton, which is typical in semileptonic $t\\bar{t}$ decays where one top quark decays leptonically, and the other decays hadronically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E7DleQT37y6"
      },
      "outputs": [],
      "source": [
        "def one_lep(lep_n):\n",
        "    # Return True if exactly one lepton in the event\n",
        "    return lep_n == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vmYaGU38WB"
      },
      "source": [
        "4. **Lepton Transverse Momentum ($p_𝑇$) Cut:**\n",
        "  * High transverse momentum leptons are a key feature of $t\\bar{t}$ events.\n",
        "  * Purpose: This cut selects events where the transverse momentum of the lepton is above 30 GeV, ensuring that only significant lepton candidates are considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1P1qI2_4hB2"
      },
      "outputs": [],
      "source": [
        "def cut_lep_pt(lep_pt):\n",
        "    # Return True if lepton pT is above 30 GeV and save a list of the lepton pT in GeV\n",
        "    return lep_pt >= 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWOAlMwZ4hfX"
      },
      "source": [
        "5. **Missing Transverse Energy ($E^{miss}_{T}$) Cut:**\n",
        "\n",
        "  * $t\\bar{t}$ events often have significant missing transverse energy due to the neutrinos produced in the decay.\n",
        "  * Purpose: This cut selects events where the missing transverse energy is greater than 30 GeV, helping to reduce background from processes without significant $E^{miss}_{T}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j0o3fdE4jNw"
      },
      "outputs": [],
      "source": [
        "def cut_met_et(met):\n",
        "    # Return True if missing ET is above 30 GeV and save a list of the missing ET in GeV\n",
        "    return met >= 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg7hrt2x4jvN"
      },
      "source": [
        "6. **W Boson Transverse Mass  ($M^{W}_{T}$) Cut:**\n",
        "  * The transverse mass of the W boson, reconstructed from the lepton transverse momentum and $E^{miss}_{T}$.\n",
        "  * Purpose: This cut ensures that the transverse mass of the W boson is above 30 GeV, further reducing background contamination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny7IW8CP4oJF"
      },
      "outputs": [],
      "source": [
        "def cut_W_mt(lep_pt, lep_phi, met_et, met_phi):\n",
        "    # Calculate W transverse mass and apply cut\n",
        "    Wmt = 2 * lep_pt * met_et * (1 - np.cos(lep_phi - met_phi))\n",
        "    cut = ak.where(Wmt > 0, np.sqrt(Wmt), 0) >= 30\n",
        "    return cut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D24AJBj4ouS"
      },
      "source": [
        "7. **Jet and b-Jet Multiplicity Cuts:**\n",
        "  * $t\\bar{t}$ events typically feature multiple jets, including those tagged as b-jets.\n",
        "  * Purpose: This cut requires at least four jets, with at least two of them identified as b-jets. This helps isolate the $t\\bar{t}$ signal from other processes that produce fewer jets or no b-jets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcsYwgHf4pGH"
      },
      "outputs": [],
      "source": [
        "def cut_Njet_and_Nbjet(jet_pt, jet_MV2c10):\n",
        "    # Count jets above pT threshold\n",
        "    num_pt = ak.sum(jet_pt >= 30, axis=1) >= 4\n",
        "\n",
        "    # Count b-tagged jets\n",
        "    num_btag = ak.sum(jet_MV2c10 >= MV2c10_lim, axis=1) >= 2\n",
        "\n",
        "    # Require at least 4 jets and 2 b-tagged jets\n",
        "    return (num_pt) & (num_btag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCJSrLwI81y_"
      },
      "source": [
        "# **Introduction to Mass Reconstruction in $t\\bar{t}$ Analysis**\n",
        "\n",
        "In the study of top-antitop quark pair production ($t\\bar{t}$), accurately reconstructing the masses of the decayed particles is essential for identifying the signal and distinguishing it from background processes. The top quark, the heaviest known elementary particle, typically decays into a W boson and a b-quark. In semileptonic $t\\bar{t}$ decays, one W boson decays into a lepton and a neutrino, while the other W boson decays hadronically into jets.\n",
        "\n",
        "To study these decays, we reconstruct the masses of the hadronically decaying top quark and the semileptonic top quark using the kinematic information of the detected particles. This process involves complex calculations that take into account the momentum, energy, and spatial distributions of the leptons, neutrinos, and jets involved in the decay.\n",
        "\n",
        "The following function, mtop, performs these calculations, reconstructing the mass of the top quarks in both their hadronic and semileptonic decay channels:\n",
        "\n",
        "**1. Leptonic Top Quark Mass ($M_{lvb}$)**\n",
        "* Purpose: The leptonic top quark mass is reconstructed using the lepton, the b-jet closest to the lepton, and the missing transverse energy (which is attributed to the neutrino).\n",
        "\n",
        "* Steps:\n",
        "  * Kinematic Conversions: Convert the energy and momentum of the lepton and neutrino from the detector units to GeV.\n",
        "\n",
        "  * Quadratic Solutions: Calculate the neutrino's longitudinal momentum ($p_{z}$) by solving a quadratic equation derived from the W boson mass constraint.\n",
        "\n",
        "  * Reconstruction: Combine the lepton, neutrino, and b-jet kinematics to compute the mass of the leptonic top quark for both solutions of $p_{z}$.\n",
        "2. Hadronic Top Quark Mass ($M_{jjj}$)\n",
        "\n",
        "* Purpose: The hadronic top quark mass is reconstructed using the b-jet farthest from the lepton and two other jets that form a pair with a mass close to that of the W boson.\n",
        "* Steps:\n",
        "  * Jet Pairing: Identify and pair jets that are not b-tagged, and calculate their invariant mass to find the pair closest to the W boson mass.\n",
        "  * Reconstruction: Combine the selected jet pair with the farthest b-jet to compute the mass of the hadronic top quark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNP7dAe39Es"
      },
      "outputs": [],
      "source": [
        "def mtop(jet_pt, jet_E, jet_eta, jet_phi, jet_MV2c10, lep_pt, lep_eta, lep_phi, lep_E, met_et, met_phi):\n",
        "    # This function calculates the mass of semileptonic decaying top quark and hadronic decay top quark\n",
        "\n",
        "    # Convert lepton kinematics\n",
        "    lep_px = lep_pt * np.cos(lep_phi)\n",
        "    lep_py = lep_pt * np.sin(lep_phi)\n",
        "    lep_pz = lep_pt / np.tan(2.0 * np.arctan( np.exp( -lep_eta ) ) )\n",
        "\n",
        "    # Convert neutrino kinematics\n",
        "    met_px = met_et * np.cos(met_phi)\n",
        "    met_py = met_et * np.sin(met_phi)\n",
        "\n",
        "    # Calculate neutrino pz solutions/\n",
        "    # Calculate coefficient 'a' of the quadratic equation\n",
        "    a = 4 * (lep_E**2 - lep_pz**2)\n",
        "\n",
        "    # Calculate coefficient 'b' of the quadratic equation, 6464.16 is (80.4 GeV)^2, which is the W boson mass squared\n",
        "    b = -4 * (6464.16 + ((lep_px + met_px)**2) +\n",
        "              ((lep_py + met_py)**2) - (lep_E**2) - (met_et**2) + (lep_pz**2)) * lep_pz\n",
        "\n",
        "    # Calculate coefficient 'c'\n",
        "    c = 4 * (lep_E**2) * (met_et**2) - (6464.16 + ((lep_px + met_px)**2) +\n",
        "               ((lep_py + met_py)**2) - (lep_E**2) - (met_et**2) + (lep_pz**2))**2\n",
        "\n",
        "    # Calculate the discriminant\n",
        "    Delta = b**2 - 4 * a * c\n",
        "\n",
        "    # Calculate the two solutions for met_pz using the quadratic formula\n",
        "    # If Delta is negative, set the solution to NaN\n",
        "    met_pz1 = ak.where(Delta >= 0, (-b + np.sqrt(Delta)) / ( 2 * a), np.NaN)\n",
        "    met_pz2 = ak.where(Delta >= 0, (-b - np.sqrt(Delta)) / ( 2 * a), np.NaN)\n",
        "\n",
        "    # Calculate the two solutions for met_E\n",
        "    met_E_1 = np.sqrt(met_px**2 + met_py**2 + met_pz1**2)\n",
        "    met_E_2 = np.sqrt(met_px**2 + met_py**2 + met_pz2**2)\n",
        "\n",
        "    # Convert jet kinematics to GeV\n",
        "    px = jet_pt * np.cos(jet_phi)\n",
        "    py = jet_pt * np.sin(jet_phi)\n",
        "    pz = jet_pt / np.tan(2.0 * np.arctan(np.exp(-jet_eta)))\n",
        "\n",
        "    # Identify b-tagged jets\n",
        "    b_tagged = jet_MV2c10 >= MV2c10_lim\n",
        "\n",
        "    # Separate b-tagged and non-b-tagged jets\n",
        "    b_tagged_px, b_tagged_py, b_tagged_pz, b_tagged_E = px[b_tagged], py[b_tagged], pz[b_tagged], jet_E[b_tagged]\n",
        "\n",
        "    b_tagged_eta, b_tagged_phi = jet_eta[b_tagged], jet_phi[b_tagged]\n",
        "\n",
        "    non_b_px, non_b_py, non_b_pz, non_b_E = px[~b_tagged], py[~b_tagged], pz[~b_tagged], jet_E[~b_tagged]\n",
        "\n",
        "\n",
        "    # Calculate delta R between lepton and b-tagged jets\n",
        "    lep_eta_broadcasted, _ = ak.broadcast_arrays(ak.flatten(lep_eta), b_tagged_eta)\n",
        "    lep_phi_broadcasted, _ = ak.broadcast_arrays(ak.flatten(lep_phi), b_tagged_phi)\n",
        "    temp_dR = (lep_eta_broadcasted - b_tagged_eta)**2 + (lep_phi_broadcasted - b_tagged_phi)**2\n",
        "    dR = ak.where(temp_dR >=0 ,np.sqrt(temp_dR),np.NaN)\n",
        "\n",
        "    # Find indices of closest and farthest b-tagged jets to the lepton\n",
        "    max_dR_indices = ak.singletons(ak.argmax(dR, axis=1))\n",
        "    min_dR_indices = ak.singletons(ak.argmin(dR, axis=1))\n",
        "\n",
        "    # Extract kinematics of closest b-tagged jets\n",
        "    closest_b_jet_E, closest_b_jet_px = b_tagged_E[min_dR_indices], b_tagged_px[min_dR_indices]\n",
        "    closest_b_jet_py, closest_b_jet_pz = b_tagged_py[min_dR_indices], b_tagged_pz[min_dR_indices]\n",
        "\n",
        "    # Extract kinematics of farthest b-tagged jets\n",
        "    farthest_b_jet_E, farthest_b_jet_px = b_tagged_E[max_dR_indices], b_tagged_px[max_dR_indices]\n",
        "    farthest_b_jet_py, farthest_b_jet_pz = b_tagged_py[max_dR_indices], b_tagged_pz[max_dR_indices]\n",
        "\n",
        "    # Helper function to create combinations of jets\n",
        "    def combo(list1):\n",
        "        jets_pairs = ak.combinations(list1, 2, fields=['List1', 'List2'])\n",
        "        sum_List = jets_pairs['List1'] + jets_pairs['List2']\n",
        "        return sum_List\n",
        "\n",
        "    # Create combinations of non-b-tagged jets\n",
        "    com_non_b_px, com_non_b_py = combo(non_b_px), combo(non_b_py)\n",
        "    com_non_b_pz, com_non_b_E = combo(non_b_pz), combo(non_b_E)\n",
        "\n",
        "    # Calculate mass difference from W boson mass\n",
        "    W_mass = np.sqrt(com_non_b_E**2 - (com_non_b_px**2 + com_non_b_py**2 + com_non_b_pz**2))\n",
        "    com_DM_W = 80.4 - W_mass\n",
        "    abs_diff = np.abs(com_DM_W)\n",
        "\n",
        "    # Select jet pairs close to W mass\n",
        "    check_abs = abs_diff <= 20\n",
        "    check_abs_array = ak.singletons(ak.any(check_abs, axis=1))\n",
        "\n",
        "    # Find best jet pair\n",
        "    min_diff_indices = ak.argmin(abs_diff, axis=1)\n",
        "    min_indices = ak.singletons(min_diff_indices)\n",
        "\n",
        "    # Extract kinematics of selected jet pair\n",
        "    sel_com_jet_E = com_non_b_E[min_indices]\n",
        "    sel_com_jet_px, sel_com_jet_py = com_non_b_px[min_indices], com_non_b_py[min_indices]\n",
        "    sel_com_jet_pz = com_non_b_pz[min_indices]\n",
        "\n",
        "    # Handle cases where no suitable jet pair is found\n",
        "    sel_com_jet_E = ak.without_parameters(ak.where(ak.num(sel_com_jet_E, axis=1) == 0, np.nan, sel_com_jet_E))\n",
        "    sel_com_jet_px = ak.without_parameters(ak.where(ak.num(sel_com_jet_px, axis=1) == 0, np.nan, sel_com_jet_px))\n",
        "    sel_com_jet_py = ak.without_parameters(ak.where(ak.num(sel_com_jet_py, axis=1) == 0, np.nan, sel_com_jet_py))\n",
        "    sel_com_jet_pz = ak.without_parameters(ak.where(ak.num(sel_com_jet_pz, axis=1) == 0, np.nan, sel_com_jet_pz))\n",
        "\n",
        "    # Calculate hadronic top\n",
        "    temp_m_jjj = ((sel_com_jet_E + farthest_b_jet_E)**2 - (sel_com_jet_px + farthest_b_jet_px)**2 -\n",
        "                  (sel_com_jet_py + farthest_b_jet_py)**2 - (sel_com_jet_pz + farthest_b_jet_pz)**2)\n",
        "\n",
        "    m_jjj = ak.where(temp_m_jjj>=0, np.sqrt(temp_m_jjj),np.nan)\n",
        "\n",
        "    # Apply W mass constraint\n",
        "    m_jjj = ak.where(check_abs_array, m_jjj, np.nan).tolist()\n",
        "\n",
        "    # Calculate leptonic top mass for both neutrino pz solutions\n",
        "    m_lvb_1 = np.sqrt((lep_E + met_E_1 + closest_b_jet_E)**2 - ( (lep_px + met_px + closest_b_jet_px)**2 +\n",
        "     (lep_py + met_py + closest_b_jet_py)**2 + (lep_pz + met_pz1 + closest_b_jet_pz)**2 ))\n",
        "\n",
        "    m_lvb_2 = np.sqrt((lep_E + met_E_2 + closest_b_jet_E)**2 - ( (lep_px + met_px + closest_b_jet_px)**2 +\n",
        "     (lep_py + met_py + closest_b_jet_py)**2 + (lep_pz + met_pz2 + closest_b_jet_pz)**2 ))\n",
        "\n",
        "    # Flatten and return results\n",
        "    m_jjj, m_lvb_1, m_lvb_2 = ak.flatten(m_jjj), ak.flatten(m_lvb_1), ak.flatten(m_lvb_2)\n",
        "    return [m_jjj, m_lvb_1, m_lvb_2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkXAGR_I7jVq"
      },
      "source": [
        "# **Data Processing and Event Selection Function**\n",
        "\n",
        "The read_file function is designed to read and process data from a file, apply selection cuts, and return an awkward array containing the events that pass all cuts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie6UR2Vc7kGW"
      },
      "outputs": [],
      "source": [
        "def read_file(path,sample,loop):\n",
        "    data_all = [] # define empty list to hold all data for this sample\n",
        "\n",
        "    # open the tree called mini using a context manager (will automatically close files/resources)\n",
        "    with uproot.open(path + f\":analysis;1\") as tree:\n",
        "        numevents = tree.num_entries  # Number of events\n",
        "        for data in tree.iterate( [\n",
        "            \"mcWeight\", \"ScaleFactor_LepTRIGGER\", \"trigE\", \"trigM\", \"lep_n\", \"lep_pt\",\"ScaleFactor_BTAG\",\n",
        "            \"lep_eta\", \"lep_phi\", \"lep_charge\", \"lep_type\", \"met\", \"met_phi\", \"sum_of_weights\", \"ScaleFactor_FTAG\",\n",
        "            \"xsec\", \"jet_pt\", \"jet_btag_quantile\", \"jet_n\", \"jet_eta\", \"jet_phi\", \"jet_e\",\"jet_jvt\",\"lep_isTrigMatched\",\n",
        "            \"lep_type\", \"lep_e\", \"eventNumber\", \"ScaleFactor_ELE\", \"ScaleFactor_MUON\",\"ScaleFactor_PILEUP\",\"filteff\", \"kfac\"],\n",
        "                                 # jet_btag_quantile\n",
        "            library=\"ak\",  # Choose output type as awkward array\n",
        "            entry_start = int( round(numevents * fraction * loop ,0) ),\n",
        "            entry_stop = int( round(numevents * fraction *(loop+1),0) )\n",
        "            ):\n",
        "\n",
        "            # Check triggerE, triggerM  conditions\n",
        "            data = data[cut_trig(data.trigE,data.trigM)]\n",
        "\n",
        "            # Require exactly one lepton\n",
        "            data = data[one_lep(data.lep_n)]\n",
        "\n",
        "            # Check lep_isTrigMatched  conditions\n",
        "            data = data[Matched_cut(data.lep_isTrigMatched)]\n",
        "\n",
        "            # Cut on lepton lep_pt\n",
        "            data = data[ak.flatten(cut_lep_pt(data.lep_pt))]\n",
        "\n",
        "            # Cut on missing transverse energy met_et\n",
        "            data = data[cut_met_et(data.met)]\n",
        "\n",
        "            data = data[ak.flatten(cut_W_mt(data.lep_pt, data.lep_phi, data.met, data.met_phi))]\n",
        "\n",
        "            data = data[cut_Njet_and_Nbjet(data.jet_pt,data.jet_btag_quantile)]\n",
        "\n",
        "            if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
        "                # multiply all Monte Carlo weights and scale factors together to give total weight\n",
        "                data['Weight'] = calc_weight(data)\n",
        "\n",
        "            else: data['Weight'] =  ak.ones_like(data['met'])\n",
        "\n",
        "            mtop_data = mtop(data.jet_pt ,data.jet_e ,data.jet_eta ,data.jet_phi ,data.jet_btag_quantile ,\n",
        "                          data.lep_pt, data.lep_eta ,data.lep_phi, data.lep_e, data.met, data.met_phi)\n",
        "\n",
        "            data['mtop'] = mtop_data[0]\n",
        "\n",
        "            data['mtop_1'] = mtop_data[1]\n",
        "\n",
        "            data['mtop_2'] = mtop_data[2]\n",
        "\n",
        "            data_all.append(data) # append array from this batch\n",
        "            del(data)\n",
        "    return ak.concatenate(data_all) # return array containing events passing all cuts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6qQ7k57kaS"
      },
      "source": [
        "# **Data Retrieval in $t\\bar{t}$ Analysis**\n",
        "\n",
        "This function plays a vital role in handling the different datasets required for a comprehensive $t\\bar{t}$ analysis, including both Monte Carlo (MC) simulation samples and actual experimental data. By organizing the data into a dictionary of awkward arrays, it ensures that the subsequent analysis steps can be performed smoothly and efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0olSZnYW7k9o"
      },
      "outputs": [],
      "source": [
        "def get_data_from_files(data_type, loop):\n",
        "    data = {} # define empty dictionary to hold awkward arrays\n",
        "    frames = [] # define empty list to hold data\n",
        "    for val in samples[data_type]['list']: # loop over each file\n",
        "        if data_type == \"data\":\n",
        "            prefix = \"Data/\"\n",
        "        else: prefix = \"MC/\"\n",
        "        #################### change to the final link #######################\n",
        "        ttuple_path = \"root://eospublic.cern.ch//eos/opendata/atlas/rucio/user/egramsta//\"\n",
        "        fileString = tuple_path+prefix+val+\".noskim.root\" # file name to open (update the skim)\n",
        "        frames.append(read_file(fileString,val,loop)) # append array returned from read_file to list of awkward arrays\n",
        "\n",
        "    data = ak.flatten(frames)\n",
        "    return data # return dictionary of awkward arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEkRLiTyYQqd"
      },
      "source": [
        "# **Data Processing**\n",
        "This function is designed to retrieve and process data for a given type (e.g., \"MC\" or \"data\") and organize it into a Pandas DataFrame. This function performs batch processing, where data is retrieved from files iteratively, allowing for efficient handling of large datasets. The DataFrame includes key variables such as event weights and the top quark masses from leptonic decays and hadronic decays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzAHJWpoYQ9j"
      },
      "outputs": [],
      "source": [
        "def analysis(data_type,loop):\n",
        "    # Process data for data_type sample\n",
        "    data = get_data_from_files(data_type,loop)\n",
        "    data_df = pd.DataFrame({\n",
        "            \"Weight\": ak.to_list(data['Weight']),\n",
        "            \"mtop\": ak.to_list(data['mtop']),\n",
        "            \"mtop2\": ak.to_list(data['mtop_1']),\n",
        "            \"mtop3\": ak.to_list(data['mtop_2'])})\n",
        "    del(data)       # Delete the 'data' dictionary to free up memory\n",
        "    return data_df  # Return the created Pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing with Parallel Execution**\n",
        "\n",
        "The parallel_analysis function is designed to improve the efficiency of data processing by utilizing parallel execution. It leverages Python’s ProcessPoolExecutor from the concurrent. futures module to execute multiple instances of the analysis function concurrently."
      ],
      "metadata": {
        "id": "uz2katz7aNJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_analysis(data_type, loop):\n",
        "    # Parallel processing\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        futures = {executor.submit(analysis, data_type, loop),\n",
        "                  executor.submit(analysis, data_type, loop+1),\n",
        "                  executor.submit(analysis, data_type, loop+2),\n",
        "                  executor.submit(analysis, data_type, loop+3)}\n",
        "\n",
        "        results = []\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                results.append(future.result())\n",
        "            except Exception as e:\n",
        "                continue\n",
        "                print(f\"Error in {data_type} loop {futures[future]}: {e}\")\n",
        "\n",
        "    # Combine results into a DataFrame\n",
        "    df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "SnPnPBAlaNeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initializing Empty DataFrames for Data Aggregation**"
      ],
      "metadata": {
        "id": "GWlWnSs0aN62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_df = pd.DataFrame()\n",
        "ttbar_df = pd.DataFrame()\n",
        "single_top_df = pd.DataFrame()\n",
        "diboson_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "M2MGI6xWaOQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHUc6f7mA3r"
      },
      "source": [
        "# **Data Aggregation with Loops**\n",
        "The code below runs over multiple datasets and concatenates the results into Pandas DataFrames. It performs this for a specified number of iterations (loops) to aggregate data from multiple files.\n",
        "\n",
        "This loop-based approach allows the processing of data in batches, making it scalable and memory-efficient for handling large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FCxJ_a9_fDN3"
      },
      "outputs": [],
      "source": [
        "Number_of_loops = 99 # If you want the analysis to go faster reduce this variable (always with an integer)\n",
        "\n",
        "print(\"The parallel analysis has started \\n\")\n",
        "start_time = time.time()\n",
        "i = 0\n",
        "\n",
        "while i < Number_of_loops:\n",
        "    Data_df = pd.concat([Data_df, parallel_analysis(\"data\",i)], ignore_index=True)\n",
        "    ttbar_df = pd.concat([ttbar_df, parallel_analysis(\"ttbar\",i)], ignore_index=True)\n",
        "    single_top_df = pd.concat([single_top_df, parallel_analysis(\"single_top\",i)], ignore_index=True)\n",
        "    diboson_df = pd.concat([diboson_df, parallel_analysis(\"diboson\",i)], ignore_index=True)\n",
        "\n",
        "    i += 4\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"{round(i*fraction*100)}% of the analysis completed in {round(elapsed_time / 60, 1)} min\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnCW2QpOlThc"
      },
      "source": [
        "# **Combining Mass and Weight Lists**\n",
        "In the context of $t\\bar{t}$ analysis, when reconstructing the mass of the top quark from the leptonic decay, we often encounter two possible solutions for the neutrino's momentum due to the nature of the quadratic equation involved in the reconstruction. These two solutions lead to two potential values for the top quark mass. To account for both possibilities in the analysis, we combine these two mass values into a single list. However, since both solutions are considered equally probable, the weight for each event must be divided by 2 to avoid double counting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyGnrgbdkvDX"
      },
      "outputs": [],
      "source": [
        "#Combine mtop2 and mtop3 lists main_analysis_df\n",
        "Data_df_2 = pd.DataFrame({\n",
        "            \"Weight\": ak.Array([1/2] * 2 * len(Data_df)),\n",
        "            \"mtop\": ak.flatten([Data_df[\"mtop2\"], Data_df[\"mtop3\"]])})\n",
        "\n",
        "# Combine MC-related lists\n",
        "ttbar_df_2 = pd.DataFrame({\n",
        "            \"Weight\": ak.flatten([ttbar_df[\"Weight\"], ttbar_df[\"Weight\"]]) / 2,\n",
        "            \"mtop\": ak.flatten([ttbar_df[\"mtop2\"], ttbar_df[\"mtop3\"]])})\n",
        "\n",
        "# Combine Di-related lists\n",
        "diboson_df_2 = pd.DataFrame({\n",
        "            \"Weight\": ak.flatten([diboson_df[\"Weight\"], diboson_df[\"Weight\"]]) / 2,\n",
        "            \"mtop\": ak.flatten([diboson_df[\"mtop2\"], diboson_df[\"mtop3\"]])})\n",
        "\n",
        "# Combine ST-related lists\n",
        "single_top_df_2 = pd.DataFrame({\n",
        "            \"Weight\": ak.flatten([single_top_df[\"Weight\"], single_top_df[\"Weight\"]]) / 2,\n",
        "            \"mtop\": ak.flatten([single_top_df[\"mtop2\"], single_top_df[\"mtop3\"]])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cuk0LiPlTQ8"
      },
      "source": [
        "# **Plotting**\n",
        "\n",
        "Define function to plot the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R81AK9L_fDK_"
      },
      "outputs": [],
      "source": [
        "def plot_data(data, data_MC, data_Di, data_ST,x_axis_label):\n",
        "    # Define plot parameters\n",
        "    xmin, xmax, step_size = 100, 250, 3\n",
        "\n",
        "    # Define MC data sets and their properties\n",
        "    datasets = [\n",
        "        {'data': data_Di['mtop'], 'weights': data_Di['Weight'], 'color': 'blue', 'label': r'Diboson'},\n",
        "        {'data': data_ST['mtop'], 'weights': data_ST['Weight'], 'color': 'cyan', 'label': r'Single top'},\n",
        "        {'data': data_MC['mtop'], 'weights': data_MC['Weight'], 'color': 'orange', 'label': r'$t\\bar{t}$'}\n",
        "    ]\n",
        "\n",
        "    # Create bin edges and centers\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = np.arange(xmin + step_size/2, xmax + step_size/2, step_size)\n",
        "\n",
        "    # Histogram the data and weights for the data\n",
        "    data_x, _ = np.histogram(ak.to_numpy(data['mtop']), bins=bin_edges, weights=data['Weight'])\n",
        "\n",
        "    data_x_errors = np.sqrt(data_x)  # statistical error on the data\n",
        "\n",
        "    # Create main plot and residual subplot\n",
        "    fig, (main_axes, residual_axes) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    # Plot data with error bars\n",
        "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
        "                       fmt='ko', label='Data')\n",
        "\n",
        "    # Plot the Monte Carlo bars\n",
        "    mc_heights = main_axes.hist([d['data'] for d in datasets], bins=bin_edges,\n",
        "                                weights=[d['weights'] for d in datasets], stacked=True,\n",
        "                                color=[d['color'] for d in datasets], label=[d['label'] for d in datasets])\n",
        "\n",
        "    mc_x_tot = (mc_heights[0][2])  # Stacked background MC y-axis value\n",
        "\n",
        "    # Calculate MC statistical uncertainty: sqrt(sum w^2)\n",
        "    mc_x_err = np.sqrt(np.histogram(np.hstack([d['data'] for d in datasets]), bins=bin_edges, weights=np.hstack([d['weights'] for d in datasets])**2)[0])\n",
        "\n",
        "    # Plot the statistical uncertainty\n",
        "    main_axes.bar(bin_centres, 2*mc_x_err, alpha=0.5, bottom=mc_x_tot-mc_x_err, color='none', hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
        "\n",
        "    # Set up main axes\n",
        "    main_axes.set_xlim(left=xmin, right=xmax)\n",
        "    main_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    main_axes.set_ylabel('Events', y=1, horizontalalignment='right')\n",
        "    main_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.set_ylim(0)\n",
        "\n",
        "    # Add text to the plot\n",
        "    main_axes.text(0.05, 0.93, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13)\n",
        "    main_axes.text(0.05, 0.88, 'for education', transform=main_axes.transAxes, style='italic', fontsize=8)\n",
        "    main_axes.text(0.05, 0.82, r'$\\sqrt{s}$=13 TeV, 36 fb$^{-1}$', transform=main_axes.transAxes)\n",
        "    main_axes.text(0.05, 0.76, r'$t\\bar{t} \\rightarrow \\ell v b\\bar{b} q\\bar{q}$', transform=main_axes.transAxes)\n",
        "\n",
        "    main_axes.legend(frameon=False)\n",
        "\n",
        "    # Calculate and plot residuals\n",
        "    ratio = data_x / (mc_heights[0][2])\n",
        "    residual_axes.errorbar(bin_centres, ratio, yerr=ratio*data_x_errors/data_x, fmt='ko')\n",
        "    residual_axes.axhline(1, color='r', linestyle='--')\n",
        "    residual_axes.set_ylim(0.5,1.5)\n",
        "    residual_axes.set_xlabel(x_axis_label, fontsize=13, x=1, horizontalalignment='right')\n",
        "    residual_axes.set_ylabel('Ratio (Data/MC)')\n",
        "    residual_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "\n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace=0.05)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFvcCpnjlYd9"
      },
      "source": [
        "Call the function to plot the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA0ew410fDHr"
      },
      "outputs": [],
      "source": [
        "# Plot the trijet mass distribution (m_jjj)\n",
        "plot_data(Data_df, ttbar_df, diboson_df, single_top_df,r\"$\\mathrm{m_{jjj}} \\ [GeV]$\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the lepton-neutrino-b-jet mass distribution (m_lvb)\n",
        "plot_data(Data_df_2, ttbar_df_2, diboson_df_2, single_top_df_2,r\"$\\mathrm{m_{lvb}} \\ [GeV]$\")"
      ],
      "metadata": {
        "id": "xeZYkYSlzCyP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
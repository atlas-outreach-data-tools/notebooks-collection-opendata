{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoPapadopoulos/Hmumu_analysis/blob/main/Hmumu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ALmDr7qOeK"
      },
      "source": [
        "# **How to search for the rare decay of a Higgs boson to Muon-Antimuon Pair ($H \\rightarrow \\mu^+ \\mu^-$) Yourself!**\n",
        "\n",
        "This notebook utilizes ATLAS Open Data to guide you through the steps necessary to search for the rare decay of a Higgs boson into a muon-antimuon pair ($H \\rightarrow \\mu^+ \\mu^-$) produced by high-energy proton-proton collisions at the Large Hadron Collider (LHC).\n",
        "\n",
        "ATLAS Open Data provides open access to proton-proton collision data recorded by the ATLAS experiment at the LHC. This dataset is available for educational purposes, making it ideal for students and educators at various levels who are interested in particle physics and various types of data analyses.\n",
        "\n",
        "\n",
        "# **What Are Notebooks?**\n",
        "\n",
        "Notebooks are interactive web applications that allow you to create and share documents that contain:\n",
        "\n",
        "1. **Live code:** Write and execute code in real-time, making adjustments as you go.\n",
        "\n",
        "2. **Visualizations:** Create plots, histograms, and other graphical representations of your data to better understand the underlying physics.\n",
        "\n",
        "3. **Narrative text:** Include explanations, descriptions, and commentary to guide yourself or others through the analysis.\n",
        "\n",
        "\n",
        "# **The Goal: Hunt for the decay H $\\rightarrow \\mu^+ \\mu^-$**\n",
        "This notebook will walk you through the process of possibly identifying and studying the decay of the Higgs boson into two muons ($ a\\ muon\\ and\\ an\\ anti-muon\\ \\mu^+ \\mu^-$). This decay channel is extremely rare, but it provides an essential probe of the interactions between the Higgs boson and second-generation fermions (leptons in this case) and muons in particular, since they are the heaviest second generation ones.\n",
        "\n",
        "By following this notebook, you will perform a $H \\rightarrow \\mu^+ \\mu^-$ search. You will apply a sequence of event selection criteria (cuts) to maximize the signal (events where $H \\rightarrow \\mu^+ \\mu^-$) to background (other processes producing pairs of muons) ratio.\n",
        "\n",
        "The process involves identifying two high-energy muons in the final state and reconstructing their invariant mass to look for a peak/bump around the Higgs boson mass of 125 GeV. The signal signature can be described as:\n",
        "\n",
        "$pp \\rightarrow H \\rightarrow \\mu^+ \\mu^-$,\n",
        "where:\n",
        "\n",
        "$\\mu^+$ is a positively charged muon(called anti-muon),\n",
        "$\\mu^-$ is a negatively charged muon.\n",
        "\n",
        "\n",
        "\n",
        "**Contents:**  \n",
        "* Running a Jupyter notebook\n",
        "* To setup\n",
        "* Explanation of Key Parameters in the $H \\rightarrow \\mu^+ \\mu^-$ Analysis Code\n",
        "* Samples\n",
        "* Weight in Particle Physics Analysis\n",
        "* Introduction to Event Selection Cuts in $H \\rightarrow \\mu^+ \\mu^-$ Analysis\n",
        "* Introduction to Mass Reconstruction in $H \\rightarrow \\mu^+ \\mu^-$ Analysis\n",
        "* Data Processing and Event Selection Function\n",
        "* Data Retrieval in $H \\rightarrow \\mu^+ \\mu^-$ Analysis\n",
        "* Data Processing\n",
        "* Data Aggregation with Loops\n",
        "* Plotting\n",
        "\n",
        "# **Running a Jupyter notebook**\n",
        "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
        "\n",
        "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
        "\n",
        "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "63X2lXpgywvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8089e7-1559-427c-d836-492c974f9089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uproot4\n",
            "  Downloading uproot4-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting uproot>=4.0.0 (from uproot4)\n",
            "  Downloading uproot-5.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting awkward>=2.4.6 (from uproot>=4.0.0->uproot4)\n",
            "  Downloading awkward-2.8.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: cramjam>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from uproot>=4.0.0->uproot4) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from uproot>=4.0.0->uproot4) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot>=4.0.0->uproot4) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot>=4.0.0->uproot4) (24.2)\n",
            "Collecting xxhash (from uproot>=4.0.0->uproot4)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting awkward-cpp==45 (from awkward>=2.4.6->uproot>=4.0.0->uproot4)\n",
            "  Downloading awkward_cpp-45-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot>=4.0.0->uproot4) (8.6.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot>=4.0.0->uproot4) (3.21.0)\n",
            "Downloading uproot4-4.0.0-py3-none-any.whl (6.2 kB)\n",
            "Downloading uproot-5.6.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.0/365.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awkward-2.8.1-py3-none-any.whl (879 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m879.4/879.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awkward_cpp-45-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (638 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m638.7/638.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, awkward-cpp, awkward, uproot, uproot4\n",
            "Successfully installed awkward-2.8.1 awkward-cpp-45 uproot-5.6.0 uproot4-4.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install uproot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2srckzeLqTOx"
      },
      "source": [
        "# **To setup**\n",
        "Cell -> Run All Below\n",
        "\n",
        "to be done every time you re-open this notebook  !!!!!!!!\n",
        "\n",
        "We're going to be using a number of tools to help us:\n",
        "\n",
        "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
        "* awkward: lets us use efficiently the nested data in columnar format\n",
        "* pandas: lets us store data as dataframes, a format widely used in python\n",
        "* numpy: provides numerical calculations such as histogramming\n",
        "* matplotlib: common tool for making plots, figures, images, visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeaI6nUjy5V3"
      },
      "outputs": [],
      "source": [
        "import uproot # For reading ROOT files efficiently\n",
        "import awkward as ak # To represent nested data in columnar format\n",
        "import pandas as pd # For dataframes, a format widely used in python\n",
        "import numpy as np # For numerical calculations such as histogramming\n",
        "import time # For timing operations and adding delays if needed\n",
        "import matplotlib.pyplot as plt # For creating plots and visualizations\n",
        "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed # Enables parallel execution for faster processing of large datasets\n",
        "# Filter warnings that otherwise appear in output. These are normal in the running of this notebook.\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in sqrt\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in power\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in multiply\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPvYOX-xqaFS"
      },
      "source": [
        "# **Explanation of Key Parameters in the $H \\rightarrow \\mu^+ \\mu^-$ Analysis Code**\n",
        "In particle physics, various parameters are crucial for correctly analyzing data and obtaining meaningful results. Below is an explanation of the key parameters used in the  $H \\rightarrow \\mu^+ \\mu^-$ analysis code:\n",
        "\n",
        "\n",
        "1. **Integrated Luminosity (lumi) Definition**\n",
        "  *   The integrated luminosity is a measure of the total data collected by the detector over a specific period. It is essential for determining the number of expected signal and background events in an analysis and is typically expressed in inverse femtobarns (fb$^{-1}$) or inverse picobarns (pb$^{-1}$). Higher luminosity means a larger statistical dataset to hunt for the rare processes like  $H \\rightarrow \\mu^+ \\mu^-$.\n",
        "2. **Fraction of Events to Process**\n",
        "  *   Definition: This parameter specifies the fraction of the total dataset to be processed by the analysis code. Reducing the fraction can be useful for testing or optimizing the analysis without processing the full dataset, saving computational time.\n",
        "\n",
        "3. For further information visit [atlas glossary](https://atlas.cern/glossary)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7G4qeaGy9p1"
      },
      "outputs": [],
      "source": [
        "# Integrated luminosity in inverse picobarns\n",
        "lumi = 36000.\n",
        "\n",
        "# Fraction of events to process\n",
        "fraction = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib_3T0e0qllU"
      },
      "source": [
        "# **Samples**\n",
        "We select these samples below to capture both the signal and relevant background processes for the Higgs boson decay to muon pairs ($H \\rightarrow \\mu^+ \\mu^-$). The data samples include all possible processes that produce a pair of muon-antinuon.  The background samples consist of simulated events chosen to account for the primary sources of noise in the analysis: the continuum Drell-Yan $\\mu^+ \\mu^-$production, which closely mimics the signal, and smaller contributions from $t\\bar{t}$ production and diboson or $W + jets$ processes. Including these samples allows us to model the background accurately and optimize cuts to enhance the signal-to-background ratio.\n",
        "\n",
        "Notice that we are using a 2 muon skim. Events have been pre-selected to include two good muons to reduce the total amount of data that must be processed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnVXcTNUql7z"
      },
      "outputs": [],
      "source": [
        "samples = {\n",
        "\n",
        "    'data': {'list' : [\"data15_periodE\",  \"data15_periodG\",  \"data16_PeriodI\",  \"data16_periodC\",  \"data16_periodF\",  \"data16_periodK\",\n",
        "                        \"data15_periodD\",  \"data15_periodH\",  \"data16_periodA\",  \"data16_periodD\",  \"data16_periodG\",  \"data16_periodL\",\n",
        "                        \"data15_periodF\",  \"data15_periodJ\",  \"data16_periodB\",  \"data16_periodE\",  \"data16_periodI\"],},\n",
        "\n",
        "    'mumu' : {  'list' : [\"mc_700323.Sh_2211_Zmumu_maxHTpTV2_BFilter\",\n",
        "                          \"mc_700324.Sh_2211_Zmumu_maxHTpTV2_CFilterBVeto\",\n",
        "                          \"mc_700325.Sh_2211_Zmumu_maxHTpTV2_CVetoBVeto\"\n",
        "                        ],},\n",
        "\n",
        "    'ttbar' : { 'list' : ['mc_410470.PhPy8EG_A14_ttbar_hdamp258p75_nonallhad'],},\n",
        "\n",
        "    'Higgs' : { 'list' : ['mc_345097.PowhegPythia8EvtGen_NNLOPS_nnlo_30_ggH125_mumu',\n",
        "                          'mc_345098.PowhegPythia8EvtGen_NNPDF3_AZNLO_ggZH125_Hmumu_Zinc',\n",
        "                          'mc_345106.PowhegPythia8EvtGen_NNPDF30_AZNLOCTEQ6L1_VBFH125_mumu' ],}\n",
        "                        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj1pw8Kbi8UL"
      },
      "source": [
        "# **Weight in Particle Physics Analysis**\n",
        "\n",
        "In particle physics analysis, such as in the study of the production of Higgs boson, the concept of \"weight\" plays a crucial role. Weights are factors applied to events or data points in a dataset to ensure that the results of an analysis accurately reflect the underlying physics being studied. These weights account for various factors, including the efficiencies of detectors, the probability of certain processes occurring, and the corrections needed to match the simulated data with real data from the LHC running.\n",
        "\n",
        "The following function computes the event weights by combining several correction factors, including trigger efficiencies, pileup corrections, b-tagging efficiencies, and cross-sections. The result is a set of weights that accurately reflect the likelihood and significance of each event, ensuring that the final analysis properly accounts for all relevant physical and experimental considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ARbGFXri9BQ"
      },
      "outputs": [],
      "source": [
        "def calc_weight(data):\n",
        "    weight_list =( data[\"ScaleFactor_MUON\"] * data[\"ScaleFactor_LepTRIGGER\"] * data[\"ScaleFactor_PILEUP\"] *\n",
        "             ( data[\"ScaleFactor_BTAG\"] * data[\"mcWeight\"] / data[\"sum_of_weights\"]) * (data[\"xsec\"] * data[\"filteff\"] * data[\"kfac\"] * lumi) )\n",
        "    return weight_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xikTX4_ENdpD"
      },
      "source": [
        "# **Introduction to Event Selection Cuts in $H \\rightarrow \\mu^+ \\mu^-$ Analysis**\n",
        "In particle physics, event selection cuts are essential to isolate signal events from overwhelming background processes. For the analysis of the Higgs boson decay into muon pairs ($H \\rightarrow \\mu^+ \\mu^-$), these cuts help filter out events that are unlikely to be associated with this rare decay mode, enhancing the likelihood of observing a signal. By applying specific selection criteria based on the physical properties of the final-state particles, we can enrich the dataset with potential signal events.\n",
        "The following are key event selection cuts used in the $H \\rightarrow \\mu^+ \\mu^-$ analysis, result from studies, aiming to maximize the signal-to-background ratio:\n",
        "\n",
        "\n",
        "1. **Trigger Selection:**\n",
        "  *   The first step in the analysis is to ensure that the events under consideration have fired the appropriate muon trigger (trigM).\n",
        "\n",
        "  *   Purpose: This cut checks whether the muon trigger has been activated for an event. If the muon trigger fired, the event is considered for further analysis, ensuring that only relevant events with potential muon candidates are processed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tfa3J2kqrC1"
      },
      "outputs": [],
      "source": [
        "def cut_trig(trigM):\n",
        "    return trigM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. **Matched Cut:**\n",
        "\n",
        "   *    This cut ensures that the event contains at least one lepton that is matched with a trigger.\n",
        "\n",
        "   *     Purpose: The cut ensures that at least one lepton in the event is associated with a trigger, meaning it was responsible for the event being recorded. Applying this cut reduces backgrounds from events where no lepton is properly matched to a trigger, improving the selection efficiency for signal events."
      ],
      "metadata": {
        "id": "Xhllb6ysbJOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Matched_cut(Matched):\n",
        "    return ak.sum(Matched == True, axis=1) >= 1"
      ],
      "metadata": {
        "id": "7SCGFZPdbJvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFaZJ1CtOZB-"
      },
      "source": [
        "  3. **Dilepton Selection:**\n",
        "\n",
        "    *   This cut ensures that the event contains exactly two leptons, which is essential for isolating signal events involving a muon pair.\n",
        "\n",
        "    *   Purpose: The cut checks if the number of leptons in the event (lep_n) is exactly equal to 2. Events with two leptons are selected for further analysis, as this is a signature of the $H \\rightarrow \\mu^+ \\mu^-$ decay, where the Higgs boson decays into a pair of muons. Events with fewer or more leptons are rejected to reduce background events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJukNjugqq-Y"
      },
      "outputs": [],
      "source": [
        "def two_lep(lep_n):\n",
        "    return lep_n == 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cUN6mDPOssC"
      },
      "source": [
        "4. **Lepton Transverse Momentum ($p_T$) Cut:**\n",
        "  * This cut ensures that both leptons in the event have sufficiently high transverse momentum ($p_T$), a key signature of particles originating from a Higgs boson decay.\n",
        "\n",
        "  * Purpose:  This cut checks if at least two leptons in the event have $p_T$ values greater than the set threshold (pt_lim = 30 GeV). Selecting leptons with high $p_T$ helps suppress background events where low-energy leptons may come from processes other than the signal. Only events with two leptons that pass the $p_T$ threshold are kept for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inoXBWQSqq2j"
      },
      "outputs": [],
      "source": [
        "def cut_lep_pt(lep_pt):\n",
        "    return ak.sum(lep_pt > 30, axis=1) >= 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAsms_FO5FU"
      },
      "source": [
        "5. **Missing Transverse Energy ($E_T^{miss}$) Cut:**\n",
        "  * This cut ensures that the missing transverse energy ($E_T^{miss}$) in the event is below a specific threshold, helping to distinguish signal events from background processes with high missing energy.\n",
        "\n",
        "   * Purpose: This cut checks if $E_T^{miss}$ is less than or equal to 80 GeV. In $H \\rightarrow \\mu^+ \\mu^-$ events, there is typically little missing energy, since the muons carry most of the momentum. Background processes involving neutrinos or other undetected particles often result in higher missing energy. By applying this cut, we reduce contamination from such background events, enhancing the purity of the selected signal events.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4ymuT46quYm"
      },
      "outputs": [],
      "source": [
        "def cut_met_et(met_et):\n",
        "    return met_et <= 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHawCvvyPGeh"
      },
      "source": [
        "6. **Opposite Charge Cut:**\n",
        "  * This cut ensures that the two leptons in the event have opposite charges.\n",
        "\n",
        "  * Purpose: The cut checks if the sum of the charges of the two leptons is zero, meaning one lepton has a positive charge and the other has a negative charge. This is consistent with the $H \\rightarrow \\mu^+ \\mu^-$ signal, where the Higgs decays into a positively charged anti-muon ($\\mu^+$) and a negatively charged muon ($\\mu^-$).This is a direct consequence of charge conservation, since the Higgs boson is neutral. Events where the lepton pair does not have opposite charges are rejected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUWWaR2kqx2r"
      },
      "outputs": [],
      "source": [
        "def cut_charge(charge):\n",
        "    return ak.sum(charge, axis=1) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMlnEGTiPG7H"
      },
      "source": [
        "7. **Lepton Type Cut:**\n",
        "  * This cut ensures that both leptons in the event are muons, which is essential for isolating the signal from the Higgs boson decay into muon pairs.\n",
        "\n",
        "  * Purpose: The cut checks if both leptons in the event are identified as muons by verifying that their type is equal to 13. The condition ensures that exactly two muons are present in the event, which is the sought after decay mode.Events containing leptons that are not muons are rejected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OhvNv_qqxzj"
      },
      "outputs": [],
      "source": [
        "def cut_type(type1):\n",
        "    return ak.sum(type1 == 13, axis=1) == 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuuSxIxdkyQQ"
      },
      "source": [
        "8. **Identification (ID) and Isolation (Iso) Cut:**\n",
        "  * This cut is designed to ensure that events selected contain particles (such as muons) that pass both identification (ID) and isolation (Iso) criteria, which are essential for selecting well-reconstructed and isolated particles.\n",
        "\n",
        "  * Purpose:\n",
        "\n",
        "    * In particle physics analyses, particularly in Higgs boson decays like\n",
        "  $H \\rightarrow \\mu^+ \\mu^-$, it is crucial to apply selection criteria based on particle identification and isolation. Identification ensures that the particles detected correspond to the expected particle type (e.g., muons), and isolation ensures that the particles are not part of a jet of particles.\n",
        "\n",
        "  * The cut performs two checks:\n",
        "\n",
        "    * Identification (ID) Check:\n",
        "  The function checks whether the total number of particles that pass the identification criteria in each event is greater than one. This ensures that the event contains at least two well-identified muons.\n",
        "    * Isolation (Iso) Check:\n",
        "  Similarly, the function checks whether the number of particles passing the isolation criteria is greater than one for each event. This ensures that multiple particles in the event are well-isolated. Non-isolated muons could be non-prompt ones coming from hadronic decays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-3QxEkIkyaO"
      },
      "outputs": [],
      "source": [
        "def ID_iso_cut(ID,iso):\n",
        "    return ak.sum(ID & iso,axis=1) > 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK9KFHJVPHNV"
      },
      "source": [
        "9. **Jet Selection Cut:**\n",
        "  * This selection cut ensures that the event contains at least two jets that satisfy specific criteria for energy, transverse momentum, and b-tagging. The two jets come from the production of the Higgs boson, normally in the Vector Boson Fusion mechanism, which is one of the dominant production mechanisms. The gluon-gluon fusion (ggF) production mechanism is more common, but can be harder to distinguish from the background because of its relatively lower jet activity.\n",
        "\n",
        "  * Purpose: The cut performs three checks:\n",
        "\n",
        "    1. **Energy Check:** Confirms that the event has at least two jets with an energy of 30 GeV or higher.\n",
        "    2. **Transverse Momentum Check:** Verifies that at least two jets have transverse momentum values exceeding 30 GeV or higher.\n",
        "    3. **b-Tagging Check:**\n",
        "      Ensures that at least two jets have b-tagging scores less than or equal to 3. Lower b-tagging scores indicate a higher probability of the jet contains b-quarks from decays of top quarks, which could contain non-prompt background muons (e.g. muons from hadron decays, rather than from Higgs bosons).\n",
        "\n",
        "    By applying this cut, we focus on events with significant jet activity, which is often associated with the production of the Higgs boson in high-energy proton-proton collisions. Events that do not meet all these criteria are rejected, thereby enhancing the likelihood of selecting signal events while suppressing background noise from processes with fewer or lower-energy jets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eazeJBncqxwJ"
      },
      "outputs": [],
      "source": [
        "def cut_jet(E,pt,b_jet):\n",
        "    return ak.sum((E >= 30) & (pt >= 30) & (b_jet <= 3), axis=1) >=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAjtDk0cPH8j"
      },
      "source": [
        "10. **Delta R ($\\Delta R$) Cut:**\n",
        "  * This cut ensures that the jets in the event are sufficiently separated from the muons in the $\\eta-\\phi$ space, which helps to reduce contamination from misidentified events.\n",
        "\n",
        "  * Purpose: The cut calculates the $\\Delta R$ values for each jet with respect to both muons using their pseudorapidities ($\\eta$) and azimuthal angles ($\\phi$):\n",
        "\n",
        "    * Calculating $\\Delta R$ for Each Muon:\n",
        "      $\\Delta R_{M1}$ is computed between each jet and the first muon.\n",
        "      $\\Delta R_{M2}$ is computed between each jet and the second muon.\n",
        "    * Separation Criteria: The cut checks if at least one jet is separated from each muon by $\\Delta R \\geq 0.4$. This separation is crucial to ensure that the jets are not too close to the muons, which could indicate that the muons are not originating from the Higgs decay but rather from other background processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7uV2BXKqxkh"
      },
      "outputs": [],
      "source": [
        "def cut_DR(eta,phi,eta_M,phi_M):\n",
        "    DR_M1 = np.sqrt((eta - eta_M[:,0])**2 + (phi -  phi_M[:,0])**2)\n",
        "    DR_M2 = np.sqrt((eta - eta_M[:,1])**2 + (phi -phi_M[:,1])**2)\n",
        "    DR_M1_check = ak.sum(DR_M1 >= 0.4, axis=1) >=1\n",
        "    DR_M2_check = ak.sum(DR_M2 >= 0.4, axis=1) >=1\n",
        "    return (DR_M1_check) & (DR_M2_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEekg69RPIN5"
      },
      "source": [
        "11. **Vector Boson Fusion (VBF) Cut:**\n",
        "\n",
        "* This cut identifies events consistent with the Vector Boson Fusion (VBF) production mechanism, where two high-energy jets are produced in association with the Higgs boson.\n",
        "\n",
        "* Purpose: The cut performs two primary checks to ensure that the event characteristics are consistent with VBF production:\n",
        "\n",
        "    1. Invariant Mass Check:\n",
        "\n",
        "  * The invariant mass of each pair of jets is calculated using their energy ($E$), transverse momentum ($p_T$), and pseudorapidity ($\\eta$) values. The invariant mass is derived from the four-momentum of the jets.\n",
        "\n",
        "  * The cut checks if at least one jet pair has an invariant mass greater than or equal to 500 GeV. This threshold is significant because it indicates the presence of high-energy jets in the forward direction typical in VBF processes.\n",
        "\n",
        "  2. Pseudorapidity Separation Check:\n",
        "  * The cut also assesses the separation in pseudorapidity between jet pairs. Specifically, it checks if the absolute difference in $\\eta$ between at least one pair of jets is greater than 3, indicating they are well-separated in rapidity.\n",
        "  * Additionally, it ensures that at least one pair of jets has one jet with a positive $\\eta$ and the other with a negative $\\eta$, confirming the jets are produced in opposite directions.\n",
        "\n",
        "* Alternate Selection Option: Consider relaxing the VBF cuts (e.g. lowering the invariant mass) or switching to a ggF-based selection. ggF normally does not have significant jet activity, so one can try a jet veto instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQg9YZuBq3P9"
      },
      "outputs": [],
      "source": [
        "def cut_VBF(jet_E,pt,eta,phi):\n",
        "    jet_px = pt * np.cos(phi)\n",
        "    jet_py = pt * np.sin(phi)\n",
        "    jet_pz = pt / np.tan(2.0 * np.arctan( np.exp( -eta ) ) )\n",
        "\n",
        "    # Helper function to create combinations of jets\n",
        "    def combo(list_1):\n",
        "        jets_pairs = ak.combinations(list_1, 2, fields=['List1', 'List2'])\n",
        "        sum_List = jets_pairs['List1'] + jets_pairs['List2']\n",
        "        return sum_List\n",
        "\n",
        "    combo_jet_E = combo(jet_E)\n",
        "    combo_jet_px = combo(jet_px)\n",
        "    combo_jet_py = combo(jet_py)\n",
        "    combo_jet_pz = combo(jet_pz)\n",
        "\n",
        "    Mass = np.sqrt(combo_jet_E**2 -(combo_jet_px**2 + combo_jet_py**2 + combo_jet_pz**2))\n",
        "\n",
        "    jets_pairs = ak.combinations(eta, 2, fields=['List1', 'List2'])\n",
        "    abs_dif_eta = np.abs(jets_pairs['List1'] - jets_pairs['List2'])\n",
        "    eta_mult = jets_pairs['List1'] * jets_pairs['List2']\n",
        "\n",
        "    return ak.sum((abs_dif_eta > 3) & (eta_mult < 0) & (Mass>=500), axis=1) > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbpqaUFCR8CK"
      },
      "source": [
        "# **Introduction to Mass Reconstruction in $H \\rightarrow \\mu^+ \\mu^-$ Analysis**\n",
        "In the analysis of Higgs boson decays into muon pairs ($H \\rightarrow \\mu^+ \\mu^-$), accurately reconstructing the mass of the Higgs boson is essential for identifying the signal and distinguishing it from various background processes. The Higgs boson, a fundamental particle responsible for giving mass to other particles, has a relatively narrow mass range centered around 125 GeV. Therefore, precise mass reconstruction is critical for effective Higgs searches.\n",
        "\n",
        "The mass reconstruction process involves using the kinematic information of the detected muons, to derive the invariant mass of the Higgs boson. This requires careful consideration of the momentum, energy, and spatial distributions of the muons and other particles produced in the collision.\n",
        "\n",
        "The following steps outline the mass reconstruction procedure for the $H \\rightarrow \\mu^+ \\mu^-$ decay channel:\n",
        "\n",
        "* **Purpose:** The leptonic Higgs mass is reconstructed using the two muons produced from the Higgs decay.\n",
        "\n",
        "\n",
        "* **Invariant Mass Calculation:** Use the four-momenta of the two muons to $$m_{μ^+μ^-}= \\sqrt{(E_{\\mu^+}+ E_{\\mu^-})^2 - (\\vec{p}_{\\mu^+} + \\vec{p}_{\\mu^-})^2}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqfOMnODq3Bp"
      },
      "outputs": [],
      "source": [
        "def Hmass(E,pt,eta,phi):\n",
        "    lep_E = ak.sum(E, axis=1)\n",
        "    lep_px = ak.sum(pt * np.cos(phi), axis=1)\n",
        "    lep_py = ak.sum(pt * np.sin(phi), axis=1)\n",
        "    lep_pz = ak.sum(pt / np.tan(2.0 * np.arctan( np.exp( -eta ) ) ), axis=1)\n",
        "    Mass = np.sqrt(lep_E**2 -(lep_px**2 + lep_py**2 + lep_pz**2))\n",
        "    return Mass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UNpOevxSd-7"
      },
      "source": [
        "# **Data Processing and Event Selection Function**\n",
        "\n",
        "The read_file function is designed to read and process data from a file, apply selection cuts, and return an awkward array containing the events that pass all cuts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQVmJwTQrpNT"
      },
      "outputs": [],
      "source": [
        "def read_file(path,sample,loop):\n",
        "    data_all = [] # define empty list to hold all data for this sample\n",
        "    # open the tree called mini using a context manager (will automatically close files/resources)\n",
        "    with uproot.open(path + f\":analysis;1\") as tree:\n",
        "        numevents = tree.num_entries # number of events\n",
        "        for data in tree.iterate([\"mcWeight\", \"ScaleFactor_MUON\", \"lep_eta\", \"lep_phi\",\"ScaleFactor_LepTRIGGER\",\n",
        "                        \"ScaleFactor_PILEUP\", \"ScaleFactor_BTAG\", \"trigE\", \"trigM\", \"lep_n\", \"lep_pt\",\n",
        "                        \"lep_charge\", \"lep_type\", \"met\", \"met_phi\",\"sum_of_weights\", \"xsec\", \"jet_pt\", \"lep_isTrigMatched\",\n",
        "                        \"lep_type\", \"lep_e\",\"num_events\",\"lep_charge\",\"jet_n\",\"jet_pt\",\"jet_eta\",\"jet_phi\",\"jet_e\",\n",
        "                        \"lep_isMediumID\",\"lep_isLooseIso\",\"jet_btag_quantile\"],\n",
        "                        library=\"ak\", # choose output type as awkward array\n",
        "                        entry_start = int(numevents * fraction * loop),\n",
        "                        entry_stop =  int(numevents*fraction* (loop + 1))\n",
        "                        ):\n",
        "\n",
        "            # Check triggerM  conditions\n",
        "            data = data[cut_trig(data.trigM)]\n",
        "\n",
        "            # Check lep_isTrigMatched  conditions\n",
        "            data = data[Matched_cut(data.lep_isTrigMatched)]\n",
        "\n",
        "            # Require exactly two lepton\n",
        "            data = data[two_lep(data.lep_n)]\n",
        "\n",
        "            # Cut on missing transverse energy met_et\n",
        "            data = data[cut_met_et(data.met)]\n",
        "\n",
        "            # Cut on type of the leptons\n",
        "            data = data[cut_type(data.lep_type)]\n",
        "\n",
        "            # cut on lepton lep_pt\n",
        "            data = data[cut_lep_pt(data.lep_pt)]\n",
        "\n",
        "            # Cut on Change of the Muons\n",
        "            data = data[cut_charge(data.lep_charge)]\n",
        "\n",
        "            # Cut on ID and Iso of leptons\n",
        "            data = data[ID_iso_cut(data.lep_isMediumID,data.lep_isLooseIso)]\n",
        "\n",
        "            # Cut on jets of the Muons\n",
        "            data = data[cut_jet(data.jet_e,data.jet_pt,data.jet_btag_quantile)]\n",
        "\n",
        "            # Cut on DR of jets with Muons\n",
        "            data = data[cut_DR(data.jet_eta,data.jet_phi,data.lep_eta,data.lep_phi)]\n",
        "\n",
        "            # Sort VBF\n",
        "            data = data[cut_VBF(data.jet_e,data.jet_pt,data.jet_eta,data.jet_phi)]\n",
        "\n",
        "            data['Inv_mass'] = Hmass(data.lep_e,data.lep_pt,data.lep_eta,data.lep_phi)\n",
        "\n",
        "            if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
        "                # multiply all Monte Carlo weights and scale factors together to give total weight\n",
        "                data['Weight'] = calc_weight(data)\n",
        "\n",
        "            else: data['Weight'] =  ak.zeros_like(data['met'])\n",
        "\n",
        "            data_all.append(data) # append array from this batch\n",
        "\n",
        "    return ak.concatenate(data_all) # return array containing events passing all cuts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJwCslSsHx"
      },
      "source": [
        "# **Data Retrieval in $H \\rightarrow \\mu^+ \\mu^-$ Analysis**\n",
        "This function is essential for managing the various datasets needed for a thorough analysis of Higgs boson decays into muon pairs ($H \\rightarrow \\mu^+ \\mu^-$). It focuses on actual experimental data collected from proton-proton collisions at the Large Hadron Collider (LHC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCHpuCeBrAVx"
      },
      "outputs": [],
      "source": [
        "def get_data_from_files(data_type, loop):\n",
        "    data = {} # define empty dictionary to hold awkward arrays\n",
        "    frames = [] # define empty list to hold data\n",
        "    for val in samples[data_type]['list']: # loop over each file\n",
        "        if data_type == \"data\":\n",
        "            prefix = \"Data/\"\n",
        "        else: prefix = \"MC/\"\n",
        "        #################### change to the final location of the files #######################\n",
        "        tuple_path = \"https://cernbox.cern.ch/files/spaces/eos/user/e/egramsta/OpenData/FEB2025/2muons/\"\n",
        "        fileString = tuple_path+prefix+val+\".2muons.root\" # file name to open\n",
        "        frames.append(read_file(fileString,val,loop)) # append array returned from read_file to list of awkward arrays\n",
        "\n",
        "    data = ak.flatten(frames)\n",
        "    return data # return dictionary of awkward arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIVlZK0sTQoR"
      },
      "source": [
        "# **Data Processing**\n",
        "The analysis function is a versatile tool designed to process data from specific datasets (data_type) in distinct iterations (loop). It efficiently extracts, organizes, and returns data in the form of a Pandas DataFrame, making it ready for further aggregation and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFQM8u1tTVFM"
      },
      "outputs": [],
      "source": [
        "def analysis(data_type,loop):\n",
        "    # Process data for data_type sample\n",
        "    data = get_data_from_files(data_type,loop)\n",
        "    data_df = pd.DataFrame({\n",
        "            \"Weight\": ak.to_list(data['Weight']),\n",
        "            \"Inv_mass\": ak.to_list(data['Inv_mass']),})\n",
        "    del(data)       # Delete the 'data' dictionary to free up memory\n",
        "    return data_df  # Return the created Pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing with Parallel Execution**\n",
        "\n",
        "The parallel_analysis function is designed to improve the efficiency of data processing by utilizing parallel execution. It leverages Python’s ProcessPoolExecutor from the concurrent. futures module to execute multiple instances of the analysis function concurrently."
      ],
      "metadata": {
        "id": "ZQV_01DtU5-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_analysis(data_type, loop):\n",
        "    # Parallel processing\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        futures = {executor.submit(analysis, data_type, loop),\n",
        "                  executor.submit(analysis, data_type, loop+1),\n",
        "                  executor.submit(analysis, data_type, loop+2),\n",
        "                  executor.submit(analysis, data_type, loop+3)}\n",
        "\n",
        "        results = []\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                results.append(future.result())\n",
        "            except Exception as e:\n",
        "                continue\n",
        "                print(f\"Error in {data_type} loop {futures[future]}: {e}\")\n",
        "\n",
        "    # Combine results into a DataFrame\n",
        "    df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
        "    return df"
      ],
      "metadata": {
        "id": "ReN3D0vmU6Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initializing Empty DataFrames for Data Aggregation**"
      ],
      "metadata": {
        "id": "JDx72GDCVCZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_df = pd.DataFrame()\n",
        "mumu_df = pd.DataFrame()\n",
        "Higgs_df = pd.DataFrame()\n",
        "ttbar_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "cpLMrzk_VB5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfzn5IsUgD-Q"
      },
      "source": [
        "# **Data Aggregation with Loops**\n",
        "The code below runs over multiple datasets and concatenates the results into Pandas DataFrames. It performs this for a specified number of iterations (loops) to aggregate data from multiple files.\n",
        "\n",
        "This loop-based approach allows the processing of data in batches, making it scalable and memory-efficient for handling large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nubi7VggE4b"
      },
      "outputs": [],
      "source": [
        "Number_of_loops = 20 # If you want the analysis to go faster reduce this variable (always with an integer)\n",
        "\n",
        "print(\"The parallel analysis has started \\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "i = 0\n",
        "\n",
        "while i < Number_of_loops:\n",
        "    Data_df = pd.concat([Data_df, parallel_analysis(\"data\",i)], ignore_index=True)\n",
        "    mumu_df = pd.concat([mumu_df, parallel_analysis(\"mumu\",i)], ignore_index=True)\n",
        "    ttbar_df = pd.concat([ttbar_df, parallel_analysis(\"ttbar\",i)], ignore_index=True)\n",
        "    Higgs_df = pd.concat([Higgs_df, parallel_analysis(\"Higgs\",i)], ignore_index=True)\n",
        "    i += 4\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"{round(i*fraction*100)}% of the analysis completed in {round(elapsed_time / 60, 1)} min \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h96KGtBRTleO"
      },
      "source": [
        "# **Plotting**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA0ew410fDHr"
      },
      "outputs": [],
      "source": [
        "def plot_data(data, data_MC, data_tt, data_H,fit):\n",
        "\n",
        "    # Define plot parameters\n",
        "    xmin, xmax, step_size = 110, 160, 2\n",
        "\n",
        "    # Define MC data sets and their properties\n",
        "    datasets = [\n",
        "        {'data': data_tt['Inv_mass'], 'weights': data_tt['Weight'], 'color': 'cyan', 'label': r'$t\\bar{t}$'},\n",
        "        {'data': data_MC['Inv_mass'], 'weights': data_MC['Weight'], 'color': 'orange', 'label': r'$μ^{-}μ^{+}$'}\n",
        "                ]\n",
        "\n",
        "    # Create bin edges and centers\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = np.arange(xmin + step_size/2, xmax + step_size/2, step_size)\n",
        "\n",
        "    # Compute the histogram of the data\n",
        "    data_x, _ = np.histogram(data['Inv_mass'], bins=bin_edges)\n",
        "\n",
        "    data_x_errors = np.sqrt(data_x)  # statistical error on the data\n",
        "\n",
        "    # Create main plot and residual subplot\n",
        "    fig, (main_axes, residual_axes) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    # Plot data with error bars\n",
        "    Cut1 = (bin_centres >= xmin) & (bin_centres <= xmax)  # Cut for main plotting range\n",
        "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors, fmt='ko', label=f'Data entries = {sum(data_x[Cut1])}')\n",
        "\n",
        "    # Plot the Monte Carlo bars\n",
        "    mc_heights = main_axes.hist([d['data'] for d in datasets], bins=bin_edges,\n",
        "                                weights=[d['weights'] for d in datasets], stacked=True,\n",
        "                                color=[d['color'] for d in datasets], label=[d['label'] for d in datasets])\n",
        "\n",
        "    mc_x_tot = (mc_heights[0][0] + mc_heights[0][1])  # Stacked background MC y-axis value\n",
        "\n",
        "    # Calculate MC statistical uncertainty: sqrt(sum w^2)\n",
        "    mc_x_err = np.sqrt(np.histogram(np.hstack([d['data'] for d in datasets]), bins=bin_edges,weights=np.hstack([d['weights'] for d in datasets])**2)[0])\n",
        "\n",
        "\n",
        "    # Plot the statistical uncertainty\n",
        "    main_axes.bar(bin_centres, 2*mc_x_err, alpha=0.5, bottom=mc_x_tot-mc_x_err,color='none', hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
        "\n",
        "    # Set up main axes\n",
        "    main_axes.set_xlim(left=xmin, right=xmax)\n",
        "\n",
        "    if fit == True:\n",
        "        higgs_hist, _ = np.histogram(data_H['Inv_mass'], bins=bin_edges, weights=data_H['Weight']*100)\n",
        "        main_axes.step(bin_centres, higgs_hist, where='mid', color='purple', linewidth=3, label='Higgs *100')\n",
        "\n",
        "\n",
        "    # Add headspace to the plot\n",
        "    ymax = max(np.max(data_x), np.max(np.sum(mc_heights[0], axis=0)))\n",
        "    main_axes.set_ylim(0, ymax * 1.4)  # Add 40% headspace\n",
        "    main_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    main_axes.set_ylabel('Events', y=1, horizontalalignment='right')\n",
        "    main_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "\n",
        "    # Add text to the plot\n",
        "    main_axes.text(0.05, 0.93, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13)\n",
        "    main_axes.text(0.05, 0.88, 'for education', transform=main_axes.transAxes, style='italic', fontsize=8)\n",
        "    #main_axes.text(0.05, 0.82, r'$\\sqrt{s}$=13 TeV, 10 fb$^{-1}$', transform=main_axes.transAxes)\n",
        "    #main_axes.text(0.05, 0.76, r'$t\\bar{t} \\rightarrow \\ell v b\\bar{b} q\\bar{q}$', transform=main_axes.transAxes)\n",
        "\n",
        "    main_axes.legend(frameon=False)\n",
        "    # Calculate and plot residuals\n",
        "    ratio = data_x / np.sum(mc_heights[0], axis=0)\n",
        "    residual_axes.errorbar(bin_centres, ratio, yerr=abs(ratio*data_x_errors/data_x), fmt='ko')\n",
        "    residual_axes.axhline(1, color='r', linestyle='--')\n",
        "    #residual_axes.set_ylim(0.5,1.5)\n",
        "    residual_axes.set_xlabel(r\"$\\mathrm{m_{μ-μ+}} \\ [GeV]$\", fontsize=13, x=1, horizontalalignment='right')\n",
        "    residual_axes.set_ylabel('Ratio (Data/MC)')\n",
        "    residual_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    residual_axes.set_ylim(top=4, bottom=0.5)\n",
        "\n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace=0.05)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_u79qVjVJyW"
      },
      "source": [
        "**Call the function to plot the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ces-iijGEB9O"
      },
      "outputs": [],
      "source": [
        "plot_data(Data_df, mumu_df, ttbar_df, Higgs_df, True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}